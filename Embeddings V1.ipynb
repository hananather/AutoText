{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "478e904e-7215-4e0e-a188-27cd9e73c894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "from nltk import edit_distance\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "716d9522-8aeb-4e9f-ba30-4868dbc7b0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute Levenshtein distance\n",
    "def levenshtein_distance(str1, str2):\n",
    "    distance = Levenshtein.distance(str1, str2)\n",
    "    max_len = max(len(str1), len(str2))\n",
    "    return 1- (distance / max_len)\n",
    "\n",
    "def n_gram_similarity(str1, str2, n=3):\n",
    "    str1_ngrams = set(ngrams(str1, n))\n",
    "    str2_ngrams = set(ngrams(str2, n))\n",
    "    return len(str1_ngrams & str2_ngrams) / float(len(str1_ngrams | str2_ngrams))\n",
    "\n",
    "def jaro_winkler_similarity(str1, str2):\n",
    "    return Levenshtein.jaro_winkler(str1, str2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f641165-4edc-49d3-87bc-30e5d8542f1e",
   "metadata": {},
   "source": [
    "**Tokenization:** The `AutoTokenizer.from_pretained(model_name)` method loads a pre-trained tokenizer. The tokenizer converts text inputs into a format that the model can process. `pt` a argument stands for PyTorch tensors. \n",
    "\n",
    "**Model Inference:** the `**` is used to unpack the dictionary `inputs` into keyword arguments. If `inputs` is a dictionary like `{'input_ids': tensor, 'attention_mask': tensor}`, then `self.model(**inputs)` is equivalent to `self.model(inputs_ids= tensor, attention_mask = tensor)`.\n",
    "\n",
    "\n",
    "**Model Output: Hidden States**\n",
    "\n",
    "When you pass a sequence of text (like a sentence) through a transformer model, output is a set of vectors. Specifically, the model produces a hidden state for each token (or sub-word) in the input sequence. Taking the mean of the hidden states is a common techique to derive a single, fixed-size vector representation for an entire input sequence. This vector can then be usedfor various downstream tasks such as similarity comparison, classification, etc..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8f0389eb-49bf-4a68-a681-4185017af105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "\n",
    "class HuggingFaceEmbedding:\n",
    "    def __init__(self, model_name = \"distilbert-base-uncased\", api_key = None):\n",
    "        # what is a tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token = api_key) \n",
    "        self.model = AutoModel.from_pretrained(model_name, use_auth_token = api_key)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors = 'pt')\n",
    "        outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim =1).detach().numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2305dade-a33c-4270-9e97-283ef766014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\n",
    "    \"HANAN TAHER TRUCKING\",\n",
    "    \"TRUCKING INC HANAN ATHER\",\n",
    "    \"ATHER TRUCKING INC\",\n",
    "    \"GODBOUT TRUCKING INC\",\n",
    "    \"HANAN ATHER PHARMACY INC\",\n",
    "    \"Ather INC\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "744ace3f-c79b-4135-8cbe-7c1b7d29f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_algorithms(records):\n",
    "    result = []\n",
    "    hf_embedding = HuggingFaceEmbedding()\n",
    "\n",
    "    for i in range(0, len(records)):\n",
    "        name1 = records[0]\n",
    "        name2 = records[i]\n",
    "        \n",
    "        lev_dist = levenshtein_distance(name1, name2)\n",
    "        ngram_sim = n_gram_similarity(name1, name2)\n",
    "        jw_sim = jaro_winkler_similarity(name1, name2)\n",
    "\n",
    "        emb1 = hf_embedding.get_embedding(name1)\n",
    "        emb2 = hf_embedding.get_embedding(name2)\n",
    "        embedding_sim = cosine_similarity(emb1, emb2)[0,0]\n",
    "\n",
    "        result.append({\n",
    "            \"Record 1\": name1,\n",
    "            \"Record 2\": name2,\n",
    "            \"Levenshtein Distance\": lev_dist,\n",
    "            \"N-Gram Similarity\": ngram_sim,\n",
    "            \"Jaro-Winkler Similarity\": jw_sim,\n",
    "            \"Embedding Similarity\": embedding_sim\n",
    "        })\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "460effe2-0749-4972-bca9-2868bc424ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Record 1</th>\n",
       "      <th>Record 2</th>\n",
       "      <th>Levenshtein Distance</th>\n",
       "      <th>N-Gram Similarity</th>\n",
       "      <th>Jaro-Winkler Similarity</th>\n",
       "      <th>Embedding Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HANAN TAHER TRUCKING</td>\n",
       "      <td>HANAN TAHER TRUCKING</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HANAN TAHER TRUCKING</td>\n",
       "      <td>TRUCKING INC HANAN ATHER</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.618254</td>\n",
       "      <td>0.898988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HANAN TAHER TRUCKING</td>\n",
       "      <td>ATHER TRUCKING INC</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.729630</td>\n",
       "      <td>0.794868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HANAN TAHER TRUCKING</td>\n",
       "      <td>GODBOUT TRUCKING INC</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.548485</td>\n",
       "      <td>0.776235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HANAN TAHER TRUCKING</td>\n",
       "      <td>HANAN ATHER PHARMACY INC</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.880833</td>\n",
       "      <td>0.888444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HANAN TAHER TRUCKING</td>\n",
       "      <td>Ather INC</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.464815</td>\n",
       "      <td>0.682862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Record 1                  Record 2  Levenshtein Distance  \\\n",
       "0  HANAN TAHER TRUCKING      HANAN TAHER TRUCKING              1.000000   \n",
       "1  HANAN TAHER TRUCKING  TRUCKING INC HANAN ATHER              0.083333   \n",
       "2  HANAN TAHER TRUCKING        ATHER TRUCKING INC              0.500000   \n",
       "3  HANAN TAHER TRUCKING      GODBOUT TRUCKING INC              0.300000   \n",
       "4  HANAN TAHER TRUCKING  HANAN ATHER PHARMACY INC              0.583333   \n",
       "5  HANAN TAHER TRUCKING                 Ather INC              0.200000   \n",
       "\n",
       "   N-Gram Similarity  Jaro-Winkler Similarity  Embedding Similarity  \n",
       "0           1.000000                 1.000000              1.000000  \n",
       "1           0.379310                 0.618254              0.898988  \n",
       "2           0.416667                 0.729630              0.794868  \n",
       "3           0.241379                 0.548485              0.776235  \n",
       "4           0.176471                 0.880833              0.888444  \n",
       "5           0.000000                 0.464815              0.682862  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = compare_algorithms(records)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfb820f-33a6-4b21-85c0-79b0627ce6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "from nltk import edit_distance\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# function to compute Levenshtein distance\n",
    "def levenshtein_distance(str1, str2):\n",
    "    distance = Levenshtein.distance(str1, str2)\n",
    "    max_len = max(len(str1), len(str2))\n",
    "    return 1- (distance / max_len)\n",
    "\n",
    "def n_gram_similarity(str1, str2, n=3):\n",
    "    str1_ngrams = set(ngrams(str1, n))\n",
    "    str2_ngrams = set(ngrams(str2, n))\n",
    "    return len(str1_ngrams & str2_ngrams) / float(len(str1_ngrams | str2_ngrams))\n",
    "\n",
    "def jaro_winkler_similarity(str1, str2):\n",
    "    return Levenshtein.jaro_winkler(str1, str2)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "\n",
    "class HuggingFaceEmbedding:\n",
    "    def __init__(self, model_name = \"distilbert-base-uncased\", api_key = None):\n",
    "        # what is a tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token = api_key) \n",
    "        self.model = AutoModel.from_pretrained(model_name, use_auth_token = api_key)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors = 'pt')\n",
    "        outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim =1).detach().numpy()\n",
    "\n",
    "\n",
    "records = [\n",
    "    \"HANAN TAHER TRUCKING\",\n",
    "    \"TRUCKING INC HANAN ATHER\",\n",
    "    \"ATHER TRUCKING INC\",\n",
    "    \"GODBOUT TRUCKING INC\",\n",
    "    \"HANAN ATHER PHARMACY INC\",\n",
    "    \"Ather INC\"\n",
    "]\n",
    "\n",
    "def compare_algorithms(records):\n",
    "    result = []\n",
    "    hf_embedding = HuggingFaceEmbedding()\n",
    "\n",
    "    for i in range(0, len(records)):\n",
    "        name1 = records[0]\n",
    "        name2 = records[i]\n",
    "        \n",
    "        lev_dist = levenshtein_distance(name1, name2)\n",
    "        ngram_sim = n_gram_similarity(name1, name2)\n",
    "        jw_sim = jaro_winkler_similarity(name1, name2)\n",
    "\n",
    "        emb1 = hf_embedding.get_embedding(name1)\n",
    "        emb2 = hf_embedding.get_embedding(name2)\n",
    "        embedding_sim = cosine_similarity(emb1, emb2)[0,0]\n",
    "\n",
    "        result.append({\n",
    "            \"Record 1\": name1,\n",
    "            \"Record 2\": name2,\n",
    "            \"Levenshtein Distance\": lev_dist,\n",
    "            \"N-Gram Similarity\": ngram_sim,\n",
    "            \"Jaro-Winkler Similarity\": jw_sim,\n",
    "            \"Embedding Similarity\": embedding_sim\n",
    "        })\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc27b4-4e8f-4183-bdb6-fa2199732d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
