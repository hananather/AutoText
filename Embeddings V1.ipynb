{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "478e904e-7215-4e0e-a188-27cd9e73c894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "from nltk import edit_distance\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "716d9522-8aeb-4e9f-ba30-4868dbc7b0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute Levenshtein distance\n",
    "def levenshtein_distance(str1, str2):\n",
    "    distance = Levenshtein.distance(str1, str2)\n",
    "    max_len = max(len(str1), len(str2))\n",
    "    return distane / max_len\n",
    "\n",
    "def n_gram_similarity(str1, str2):\n",
    "    str1_ngrams = set(ngrams(str1, n))\n",
    "    str2_ngrams = set(ngrams(str2, n))\n",
    "    return len(str1_ngrams & str2_ngrams) / float(len(str1_ngrams | str2_ngrams))\n",
    "\n",
    "def naro_winkler(str1, str2):\n",
    "    return Levenshtein.jaro_winkler(str1, str2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f641165-4edc-49d3-87bc-30e5d8542f1e",
   "metadata": {},
   "source": [
    "**Tokenization:** The `AutoTokenizer.from_pretained(model_name)` method loads a pre-trained tokenizer. The tokenizer converts text inputs into a format that the model can process. `pt` a argument stands for PyTorch tensors. \n",
    "\n",
    "**Model Inference:** the `**` is used to unpack the dictionary `inputs` into keyword arguments. If `inputs` is a dictionary like `{'input_ids': tensor, 'attention_mask': tensor}`, then `self.model(**inputs)` is equivalent to `self.model(inputs_ids= tensor, attention_mask = tensor)`.\n",
    "\n",
    "\n",
    "**Model Output: Hidden States**\n",
    "\n",
    "When you pass a sequence of text (like a sentence) through a transformer model, output is a set of vectors. Specifically, the model produces a hidden state for each token (or sub-word) in the input sequence. Taking the mean of the hidden states is a common techique to derive a single, fixed-size vector representation for an entire input sequence. This vector can then be usedfor various downstream tasks such as similarity comparison, classification, etc..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f0389eb-49bf-4a68-a681-4185017af105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "\n",
    "class HuggingFaceEmbedding:\n",
    "    def __init__(self, model_name = \"distilbert-base-uncased\", api_key = None):\n",
    "        # what is a tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token = api_key) \n",
    "        self.model = AutoModel.from_pretrained(model_name, use_auth_token = api_key)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensor = 'pt')\n",
    "        outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim =1).detach().numpy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2305dade-a33c-4270-9e97-283ef766014d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
