{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42dab2bd-9546-4c11-9691-076edbc2e9ab",
   "metadata": {},
   "source": [
    "## Imports and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b7fe590-bfd9-4baa-99a5-e0e9a2841074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "from nltk.util import ngrams\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Function to compute Levenshtein distance\n",
    "def levenshtein_distance(str1, str2):\n",
    "    distance = Levenshtein.distance(str1, str2)\n",
    "    max_len = max(len(str1), len(str2))\n",
    "    return 1 - (distance / max_len)\n",
    "\n",
    "# Function to compute N-gram similarity\n",
    "def n_gram_similarity(str1, str2, n=3):\n",
    "    str1_ngrams = set(ngrams(str1, n))\n",
    "    str2_ngrams = set(ngrams(str2, n))\n",
    "    return len(str1_ngrams & str2_ngrams) / float(len(str1_ngrams | str2_ngrams))\n",
    "\n",
    "# Function to compute Jaro-Winkler similarity\n",
    "def jaro_winkler_similarity(str1, str2):\n",
    "    return Levenshtein.jaro_winkler(str1, str2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a678976d-117b-490f-b137-955cae4f715f",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8de66595-19d6-4b4e-aab6-9acf30f2ed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for business names\n",
    "class BusinessNamesDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=128):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name1 = self.data.iloc[idx]['name1']\n",
    "        name2 = self.data.iloc[idx]['name2']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        inputs = self.tokenizer(name1, name2, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
    "        input_ids = inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "        return input_ids, attention_mask, torch.tensor(label, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808212c8-a46c-454d-b0a0-4b6d2166ced8",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca08fe47-e947-405e-990e-e22c032d5aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class for business names similarity\n",
    "class BusinessNamesModel(torch.nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(BusinessNamesModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.similarity = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)  # Pool the last hidden state\n",
    "        similarity_score = self.similarity(pooled_output)\n",
    "        return similarity_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf2db7-1305-4c64-83e2-9e31f1633736",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "391c2f15-bc77-46e8-b8f5-81e51cbb10c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "def train_model(model, train_dataloader, val_dataloader=None, num_epochs=3, learning_rate=2e-5):\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs.squeeze(-1), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_dataloader)}')\n",
    "\n",
    "        # Validation\n",
    "        if val_dataloader:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_dataloader:\n",
    "                    input_ids, attention_mask, labels = batch\n",
    "                    input_ids = input_ids.to(device)\n",
    "                    attention_mask = attention_mask.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    \n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    loss = criterion(outputs.squeeze(-1), labels.float())\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Validation Loss: {val_loss/len(val_dataloader)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d5cb5-4e99-43c1-b87f-c898b534a9ff",
   "metadata": {},
   "source": [
    "## Pre-trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "301ed86f-2aae-4204-940e-77c272ff2ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for pre-trained embeddings\n",
    "class PreTrainedEmbedding:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef3ddf7-3966-4950-b24f-eaa3baaaaeae",
   "metadata": {},
   "source": [
    "## Fine-Tuned Embedding Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bb0facc-6414-447a-8519-caa60077f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get fine-tuned embeddings\n",
    "class FineTunedEmbedding:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.bert(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d62a3ea-5f25-4581-97cd-1129e9b4c522",
   "metadata": {},
   "source": [
    "## Comparing Business Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65d246e7-4f8f-41f0-8e62-d43f83ad14e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare different algorithms\n",
    "def compare_algorithms(records, pre_trained_embedding, fine_tuned_embedding):\n",
    "    result = []\n",
    "    for i in range(len(records)):\n",
    "        for j in range(i + 1, len(records)):\n",
    "            name1 = records[i]\n",
    "            name2 = records[j]\n",
    "            \n",
    "            lev_dist = levenshtein_distance(name1, name2)\n",
    "            ngram_sim = n_gram_similarity(name1, name2)\n",
    "            jw_sim = jaro_winkler_similarity(name1, name2)\n",
    "\n",
    "            pre_emb1 = pre_trained_embedding.get_embedding(name1)\n",
    "            pre_emb2 = pre_trained_embedding.get_embedding(name2)\n",
    "            pre_embedding_sim = cosine_similarity(pre_emb1, pre_emb2)[0, 0]\n",
    "\n",
    "            fine_emb1 = fine_tuned_embedding.get_embedding(name1)\n",
    "            fine_emb2 = fine_tuned_embedding.get_embedding(name2)\n",
    "            fine_embedding_sim = cosine_similarity(fine_emb1, fine_emb2)[0, 0]\n",
    "\n",
    "            result.append({\n",
    "                \"Record 1\": name1,\n",
    "                \"Record 2\": name2,\n",
    "                \"Levenshtein Distance\": lev_dist,\n",
    "                \"N-Gram Similarity\": ngram_sim,\n",
    "                \"Jaro-Winkler Similarity\": jw_sim,\n",
    "                \"Pre-trained Embedding Similarity\": pre_embedding_sim,\n",
    "                \"Fine-tuned Embedding Similarity\": fine_embedding_sim\n",
    "            })\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4b9b55-a3de-48c2-a861-ef1707be6d14",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f7f3d09-bbed-48c6-b27c-0a69f6bacde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hananather/Desktop/AutoText/venv/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7044497132301331\n",
      "Epoch 1, Validation Loss: 0.6895326972007751\n",
      "Epoch 2, Loss: 0.6726543307304382\n",
      "Epoch 2, Validation Loss: 0.6965304017066956\n",
      "Epoch 3, Loss: 0.6389493942260742\n",
      "Epoch 3, Validation Loss: 0.7096562385559082\n",
      "                    Record 1                  Record 2  Levenshtein Distance  \\\n",
      "0       HANAN TAHER TRUCKING  TRUCKING INC HANAN ATHER              0.083333   \n",
      "1       HANAN TAHER TRUCKING        ATHER TRUCKING INC              0.500000   \n",
      "2       HANAN TAHER TRUCKING      GODBOUT TRUCKING INC              0.300000   \n",
      "3       HANAN TAHER TRUCKING  HANAN ATHER PHARMACY INC              0.583333   \n",
      "4       HANAN TAHER TRUCKING                 Ather INC              0.200000   \n",
      "5   TRUCKING INC HANAN ATHER        ATHER TRUCKING INC              0.250000   \n",
      "6   TRUCKING INC HANAN ATHER      GODBOUT TRUCKING INC              0.166667   \n",
      "7   TRUCKING INC HANAN ATHER  HANAN ATHER PHARMACY INC              0.083333   \n",
      "8   TRUCKING INC HANAN ATHER                 Ather INC              0.166667   \n",
      "9         ATHER TRUCKING INC      GODBOUT TRUCKING INC              0.650000   \n",
      "10        ATHER TRUCKING INC  HANAN ATHER PHARMACY INC              0.416667   \n",
      "11        ATHER TRUCKING INC                 Ather INC              0.277778   \n",
      "12      GODBOUT TRUCKING INC  HANAN ATHER PHARMACY INC              0.250000   \n",
      "13      GODBOUT TRUCKING INC                 Ather INC              0.200000   \n",
      "14  HANAN ATHER PHARMACY INC                 Ather INC              0.208333   \n",
      "\n",
      "    N-Gram Similarity  Jaro-Winkler Similarity  \\\n",
      "0            0.379310                 0.618254   \n",
      "1            0.416667                 0.729630   \n",
      "2            0.241379                 0.548485   \n",
      "3            0.176471                 0.880833   \n",
      "4            0.000000                 0.464815   \n",
      "5            0.520000                 0.644180   \n",
      "6            0.333333                 0.576709   \n",
      "7            0.333333                 0.616667   \n",
      "8            0.074074                 0.453704   \n",
      "9            0.478261                 0.802116   \n",
      "10           0.187500                 0.626425   \n",
      "11           0.095238                 0.544444   \n",
      "12           0.052632                 0.497222   \n",
      "13           0.086957                 0.464815   \n",
      "14           0.074074                 0.453704   \n",
      "\n",
      "    Pre-trained Embedding Similarity  Fine-tuned Embedding Similarity  \n",
      "0                           0.898988                         0.917343  \n",
      "1                           0.794868                         0.801414  \n",
      "2                           0.776235                         0.778995  \n",
      "3                           0.888444                         0.888637  \n",
      "4                           0.682862                         0.695386  \n",
      "5                           0.893964                         0.892252  \n",
      "6                           0.822202                         0.816756  \n",
      "7                           0.886467                         0.902437  \n",
      "8                           0.820383                         0.815225  \n",
      "9                           0.919661                         0.918132  \n",
      "10                          0.847425                         0.852005  \n",
      "11                          0.866637                         0.874409  \n",
      "12                          0.780057                         0.780687  \n",
      "13                          0.747102                         0.747772  \n",
      "14                          0.794609                         0.799756  \n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "pre_trained_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "pre_trained_embedding = PreTrainedEmbedding(pre_trained_model, tokenizer)\n",
    "\n",
    "# Load dataset\n",
    "dataset = BusinessNamesDataset('test_business_names.csv', tokenizer)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Initialize and train model\n",
    "fine_tuned_model = BusinessNamesModel(\"distilbert-base-uncased\")\n",
    "train_model(fine_tuned_model, train_dataloader, val_dataloader)\n",
    "\n",
    "# Initialize fine-tuned embedding class\n",
    "fine_tuned_embedding = FineTunedEmbedding(fine_tuned_model, tokenizer)\n",
    "\n",
    "\n",
    "# Define business names to compare\n",
    "records = [\n",
    "    \"HANAN TAHER TRUCKING\",\n",
    "    \"TRUCKING INC HANAN ATHER\",\n",
    "    \"ATHER TRUCKING INC\",\n",
    "    \"GODBOUT TRUCKING INC\",\n",
    "    \"HANAN ATHER PHARMACY INC\",\n",
    "    \"Ather INC\"\n",
    "]\n",
    "\n",
    "# Compare business names and print results\n",
    "results = compare_algorithms(records, pre_trained_embedding, fine_tuned_embedding)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a09e5-53f8-48bc-9f5f-073a668e9bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
