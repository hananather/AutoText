{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7fe590-bfd9-4baa-99a5-e0e9a2841074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "from nltk.util import ngrams\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Function to compute Levenshtein distance\n",
    "def levenshtein_distance(str1, str2):\n",
    "    distance = Levenshtein.distance(str1, str2)\n",
    "    max_len = max(len(str1), len(str2))\n",
    "    return 1 - (distance / max_len)\n",
    "\n",
    "def n_gram_similarity(str1, str2, n=3):\n",
    "    str1_ngrams = set(ngrams(str1, n))\n",
    "    str2_ngrams = set(ngrams(str2, n))\n",
    "    return len(str1_ngrams & str2_ngrams) / float(len(str1_ngrams | str2_ngrams))\n",
    "\n",
    "def jaro_winkler_similarity(str1, str2):\n",
    "    return Levenshtein.jaro_winkler(str1, str2)\n",
    "\n",
    "# Dataset class for business names\n",
    "class BusinessNamesDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=128):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name1 = self.data.iloc[idx]['name1']\n",
    "        name2 = self.data.iloc[idx]['name2']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        inputs = self.tokenizer(name1, name2, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
    "        input_ids = inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "        return input_ids, attention_mask, torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "# Model class for business names similarity\n",
    "class BusinessNamesModel(torch.nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(BusinessNamesModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.similarity = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]  # Assuming the model returns (sequence_output, pooled_output)\n",
    "        similarity_score = self.similarity(pooled_output)\n",
    "        return similarity_score\n",
    "\n",
    "# Training the model\n",
    "def train_model(model, dataloader, num_epochs=3, learning_rate=2e-5):\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs.squeeze(-1), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}')\n",
    "\n",
    "# Function to get fine-tuned embeddings\n",
    "class FineTunedEmbedding:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.bert(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# Function to compare different algorithms\n",
    "def compare_algorithms(records, fine_tuned_embedding):\n",
    "    result = []\n",
    "    for i in range(len(records)):\n",
    "        name1 = records[0]\n",
    "        name2 = records[i]\n",
    "        \n",
    "        lev_dist = levenshtein_distance(name1, name2)\n",
    "        ngram_sim = n_gram_similarity(name1, name2)\n",
    "        jw_sim = jaro_winkler_similarity(name1, name2)\n",
    "\n",
    "        emb1 = fine_tuned_embedding.get_embedding(name1)\n",
    "        emb2 = fine_tuned_embedding.get_embedding(name2)\n",
    "        embedding_sim = cosine_similarity(emb1, emb2)[0, 0]\n",
    "\n",
    "        result.append({\n",
    "            \"Record 1\": name1,\n",
    "            \"Record 2\": name2,\n",
    "            \"Levenshtein Distance\": lev_dist,\n",
    "            \"N-Gram Similarity\": ngram_sim,\n",
    "            \"Jaro-Winkler Similarity\": jw_sim,\n",
    "            \"Embedding Similarity\": embedding_sim\n",
    "        })\n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "# Main execution\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "dataset = BusinessNamesDataset('business_names.csv', tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "model = BusinessNamesModel(\"distilbert-base-uncased\")\n",
    "train_model(model, dataloader)\n",
    "\n",
    "fine_tuned_embedding = FineTunedEmbedding(model, tokenizer)\n",
    "\n",
    "records = [\n",
    "    \"HANAN TAHER TRUCKING\",\n",
    "    \"TRUCKING INC HANAN ATHER\",\n",
    "    \"ATHER TRUCKING INC\",\n",
    "    \"GODBOUT TRUCKING INC\",\n",
    "    \"HANAN ATHER PHARMACY INC\",\n",
    "    \"Ather INC\"\n",
    "]\n",
    "\n",
    "results = compare_algorithms(records, fine_tuned_embedding)\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
