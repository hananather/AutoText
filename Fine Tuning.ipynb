{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42dab2bd-9546-4c11-9691-076edbc2e9ab",
   "metadata": {},
   "source": [
    "## Imports and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b7fe590-bfd9-4baa-99a5-e0e9a2841074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "from nltk.util import ngrams\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Function to compute Levenshtein distance\n",
    "def levenshtein_distance(str1, str2):\n",
    "    distance = Levenshtein.distance(str1, str2)\n",
    "    max_len = max(len(str1), len(str2))\n",
    "    return 1 - (distance / max_len)\n",
    "\n",
    "# Function to compute N-gram similarity\n",
    "def n_gram_similarity(str1, str2, n=3):\n",
    "    str1_ngrams = set(ngrams(str1, n))\n",
    "    str2_ngrams = set(ngrams(str2, n))\n",
    "    return len(str1_ngrams & str2_ngrams) / float(len(str1_ngrams | str2_ngrams))\n",
    "\n",
    "# Function to compute Jaro-Winkler similarity\n",
    "def jaro_winkler_similarity(str1, str2):\n",
    "    return Levenshtein.jaro_winkler(str1, str2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a678976d-117b-490f-b137-955cae4f715f",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8de66595-19d6-4b4e-aab6-9acf30f2ed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for business names\n",
    "class BusinessNamesDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=128):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name1 = self.data.iloc[idx]['name1']\n",
    "        name2 = self.data.iloc[idx]['name2']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        inputs = self.tokenizer(name1, name2, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
    "        input_ids = inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "        return input_ids, attention_mask, torch.tensor(label, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808212c8-a46c-454d-b0a0-4b6d2166ced8",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca08fe47-e947-405e-990e-e22c032d5aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class for business names similarity\n",
    "class BusinessNamesModel(torch.nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(BusinessNamesModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.similarity = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)  # Pool the last hidden state\n",
    "        similarity_score = self.similarity(pooled_output)\n",
    "        return similarity_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf2db7-1305-4c64-83e2-9e31f1633736",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "391c2f15-bc77-46e8-b8f5-81e51cbb10c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "def train_model(model, train_dataloader, val_dataloader=None, num_epochs=3, learning_rate=2e-5):\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs.squeeze(-1), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_dataloader)}')\n",
    "\n",
    "        # Validation\n",
    "        if val_dataloader:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_dataloader:\n",
    "                    input_ids, attention_mask, labels = batch\n",
    "                    input_ids = input_ids.to(device)\n",
    "                    attention_mask = attention_mask.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    \n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    loss = criterion(outputs.squeeze(-1), labels.float())\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Validation Loss: {val_loss/len(val_dataloader)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d5cb5-4e99-43c1-b87f-c898b534a9ff",
   "metadata": {},
   "source": [
    "## Pre-trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "301ed86f-2aae-4204-940e-77c272ff2ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for pre-trained embeddings\n",
    "class PreTrainedEmbedding:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef3ddf7-3966-4950-b24f-eaa3baaaaeae",
   "metadata": {},
   "source": [
    "## Fine-Tuned Embedding Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bb0facc-6414-447a-8519-caa60077f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get fine-tuned embeddings\n",
    "class FineTunedEmbedding:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.bert(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d62a3ea-5f25-4581-97cd-1129e9b4c522",
   "metadata": {},
   "source": [
    "## Comparing Business Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65d246e7-4f8f-41f0-8e62-d43f83ad14e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare different algorithms\n",
    "def compare_algorithms(records, pre_trained_embedding, fine_tuned_embedding):\n",
    "    result = []\n",
    "    for i in range(len(records)):\n",
    "        for j in range(i + 1, len(records)):\n",
    "            name1 = records[i]\n",
    "            name2 = records[j]\n",
    "            \n",
    "            lev_dist = levenshtein_distance(name1, name2)\n",
    "            ngram_sim = n_gram_similarity(name1, name2)\n",
    "            jw_sim = jaro_winkler_similarity(name1, name2)\n",
    "\n",
    "            pre_emb1 = pre_trained_embedding.get_embedding(name1)\n",
    "            pre_emb2 = pre_trained_embedding.get_embedding(name2)\n",
    "            pre_embedding_sim = cosine_similarity(pre_emb1, pre_emb2)[0, 0]\n",
    "\n",
    "            fine_emb1 = fine_tuned_embedding.get_embedding(name1)\n",
    "            fine_emb2 = fine_tuned_embedding.get_embedding(name2)\n",
    "            fine_embedding_sim = cosine_similarity(fine_emb1, fine_emb2)[0, 0]\n",
    "\n",
    "            result.append({\n",
    "                \"Record 1\": name1,\n",
    "                \"Record 2\": name2,\n",
    "                \"Levenshtein Distance\": lev_dist,\n",
    "                \"N-Gram Similarity\": ngram_sim,\n",
    "                \"Jaro-Winkler Similarity\": jw_sim,\n",
    "                \"Pre-trained Embedding Similarity\": pre_embedding_sim,\n",
    "                \"Fine-tuned Embedding Similarity\": fine_embedding_sim\n",
    "            })\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4b9b55-a3de-48c2-a861-ef1707be6d14",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f7f3d09-bbed-48c6-b27c-0a69f6bacde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hananather/Desktop/AutoText/venv/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7044497132301331\n",
      "Epoch 1, Validation Loss: 0.6895326972007751\n",
      "Epoch 2, Loss: 0.6726543307304382\n",
      "Epoch 2, Validation Loss: 0.6965304017066956\n",
      "Epoch 3, Loss: 0.6389493942260742\n",
      "Epoch 3, Validation Loss: 0.7096562385559082\n",
      "                    Record 1                  Record 2  Levenshtein Distance  \\\n",
      "0       HANAN TAHER TRUCKING  TRUCKING INC HANAN ATHER              0.083333   \n",
      "1       HANAN TAHER TRUCKING        ATHER TRUCKING INC              0.500000   \n",
      "2       HANAN TAHER TRUCKING      GODBOUT TRUCKING INC              0.300000   \n",
      "3       HANAN TAHER TRUCKING  HANAN ATHER PHARMACY INC              0.583333   \n",
      "4       HANAN TAHER TRUCKING                 Ather INC              0.200000   \n",
      "5   TRUCKING INC HANAN ATHER        ATHER TRUCKING INC              0.250000   \n",
      "6   TRUCKING INC HANAN ATHER      GODBOUT TRUCKING INC              0.166667   \n",
      "7   TRUCKING INC HANAN ATHER  HANAN ATHER PHARMACY INC              0.083333   \n",
      "8   TRUCKING INC HANAN ATHER                 Ather INC              0.166667   \n",
      "9         ATHER TRUCKING INC      GODBOUT TRUCKING INC              0.650000   \n",
      "10        ATHER TRUCKING INC  HANAN ATHER PHARMACY INC              0.416667   \n",
      "11        ATHER TRUCKING INC                 Ather INC              0.277778   \n",
      "12      GODBOUT TRUCKING INC  HANAN ATHER PHARMACY INC              0.250000   \n",
      "13      GODBOUT TRUCKING INC                 Ather INC              0.200000   \n",
      "14  HANAN ATHER PHARMACY INC                 Ather INC              0.208333   \n",
      "\n",
      "    N-Gram Similarity  Jaro-Winkler Similarity  \\\n",
      "0            0.379310                 0.618254   \n",
      "1            0.416667                 0.729630   \n",
      "2            0.241379                 0.548485   \n",
      "3            0.176471                 0.880833   \n",
      "4            0.000000                 0.464815   \n",
      "5            0.520000                 0.644180   \n",
      "6            0.333333                 0.576709   \n",
      "7            0.333333                 0.616667   \n",
      "8            0.074074                 0.453704   \n",
      "9            0.478261                 0.802116   \n",
      "10           0.187500                 0.626425   \n",
      "11           0.095238                 0.544444   \n",
      "12           0.052632                 0.497222   \n",
      "13           0.086957                 0.464815   \n",
      "14           0.074074                 0.453704   \n",
      "\n",
      "    Pre-trained Embedding Similarity  Fine-tuned Embedding Similarity  \n",
      "0                           0.898988                         0.917343  \n",
      "1                           0.794868                         0.801414  \n",
      "2                           0.776235                         0.778995  \n",
      "3                           0.888444                         0.888637  \n",
      "4                           0.682862                         0.695386  \n",
      "5                           0.893964                         0.892252  \n",
      "6                           0.822202                         0.816756  \n",
      "7                           0.886467                         0.902437  \n",
      "8                           0.820383                         0.815225  \n",
      "9                           0.919661                         0.918132  \n",
      "10                          0.847425                         0.852005  \n",
      "11                          0.866637                         0.874409  \n",
      "12                          0.780057                         0.780687  \n",
      "13                          0.747102                         0.747772  \n",
      "14                          0.794609                         0.799756  \n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "pre_trained_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "pre_trained_embedding = PreTrainedEmbedding(pre_trained_model, tokenizer)\n",
    "\n",
    "# Load dataset\n",
    "dataset = BusinessNamesDataset('test_business_names.csv', tokenizer)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Initialize and train model\n",
    "fine_tuned_model = BusinessNamesModel(\"distilbert-base-uncased\")\n",
    "train_model(fine_tuned_model, train_dataloader, val_dataloader)\n",
    "\n",
    "# Initialize fine-tuned embedding class\n",
    "fine_tuned_embedding = FineTunedEmbedding(fine_tuned_model, tokenizer)\n",
    "\n",
    "\n",
    "# Define business names to compare\n",
    "records = [\n",
    "    \"HANAN TAHER TRUCKING\",\n",
    "    \"TRUCKING INC HANAN ATHER\",\n",
    "    \"ATHER TRUCKING INC\",\n",
    "    \"GODBOUT TRUCKING INC\",\n",
    "    \"HANAN ATHER PHARMACY INC\",\n",
    "    \"Ather INC\"\n",
    "]\n",
    "\n",
    "# Compare business names and print results\n",
    "results = compare_algorithms(records, pre_trained_embedding, fine_tuned_embedding)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "128a09e5-53f8-48bc-9f5f-073a668e9bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(61308) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Business Name 1           Business Name 2  Levenshtein    N-gram  \\\n",
      "0  HANAN ATHER TRUCKING      HANAN TAHER TRUCKING     0.900000  0.636364   \n",
      "1  HANAN ATHER TRUCKING  TRUCKING INC HANAN ATHER     0.083333  0.600000   \n",
      "2  HANAN ATHER TRUCKING        ATHER TRUCKING INC     0.500000  0.545455   \n",
      "3  HANAN ATHER TRUCKING      GODBOUT TRUCKING INC     0.300000  0.241379   \n",
      "4  HANAN ATHER TRUCKING  HANAN ATHER PHARMACY INC     0.666667  0.333333   \n",
      "5  HANAN ATHER TRUCKING                 Ather INC     0.400000  0.190476   \n",
      "\n",
      "   Jaro-Winkler  SBERT_Pre-fine-tuned  RoBERTa_Pre-fine-tuned  \\\n",
      "0      0.990000              0.911981                0.913551   \n",
      "1      0.618254              0.930841                0.876942   \n",
      "2      0.729630              0.725133                0.701722   \n",
      "3      0.548485              0.587563                0.523199   \n",
      "4      0.893333              0.590314                0.497729   \n",
      "5      0.637963              0.483883                0.419634   \n",
      "\n",
      "   Mini_Pre-fine-tuned  OpenAI_text-embedding-3-small  \\\n",
      "0             0.929135                       0.888799   \n",
      "1             0.875216                       0.888552   \n",
      "2             0.642829                       0.752530   \n",
      "3             0.465248                       0.570155   \n",
      "4             0.561207                       0.693619   \n",
      "5             0.357233                       0.350427   \n",
      "\n",
      "   OpenAI_text-embedding-3-large  OpenAI_text-embedding-ada-002  \n",
      "0                       0.898423                       0.968436  \n",
      "1                       0.896059                       0.963726  \n",
      "2                       0.671729                       0.877687  \n",
      "3                       0.539397                       0.860756  \n",
      "4                       0.698483                       0.904037  \n",
      "5                       0.384743                       0.815603  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import Levenshtein\n",
    "from nltk.util import ngrams\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Ensure we're using CPU\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Your provided dataset\n",
    "businessNames1 = [\"HANAN ATHER TRUCKING\"]\n",
    "businessNames2 = [\n",
    "    \"HANAN TAHER TRUCKING\",\n",
    "    \"TRUCKING INC HANAN ATHER\",\n",
    "    \"ATHER TRUCKING INC\",\n",
    "    \"GODBOUT TRUCKING INC\",\n",
    "    \"HANAN ATHER PHARMACY INC\",\n",
    "    \"Ather INC\"\n",
    "]\n",
    "\n",
    "def evaluate_model_on_dataset(model, names1, names2):\n",
    "    with torch.no_grad():\n",
    "        embeddings1 = model.encode(names1, convert_to_tensor=True, device=device)\n",
    "        embeddings2 = model.encode(names2, convert_to_tensor=True, device=device)\n",
    "    \n",
    "    similarities = torch.nn.functional.cosine_similarity(embeddings1, embeddings2).cpu().numpy()\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "def levenshtein_distance(str1, str2):\n",
    "    distance = Levenshtein.distance(str1.lower(), str2.lower())\n",
    "    max_len = max(len(str1), len(str2))\n",
    "    return 1 - (distance / max_len)\n",
    "\n",
    "def n_gram_similarity(str1, str2, n=3):\n",
    "    str1_ngrams = set(ngrams(str1.lower(), n))\n",
    "    str2_ngrams = set(ngrams(str2.lower(), n))\n",
    "    return len(str1_ngrams & str2_ngrams) / float(len(str1_ngrams | str2_ngrams))\n",
    "\n",
    "def jaro_winkler_similarity(str1, str2):\n",
    "    return Levenshtein.jaro_winkler(str1.lower(), str2.lower())\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "models = {\n",
    "    'SBERT': 'all-mpnet-base-v2',\n",
    "    'RoBERTa': 'all-distilroberta-v1',\n",
    "    'Mini': 'all-MiniLM-L6-v2',\n",
    "}\n",
    "\n",
    "openai_models = [\n",
    "    'text-embedding-3-small',\n",
    "    'text-embedding-3-large',\n",
    "    'text-embedding-ada-002'\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Get embeddings for businessNames1\n",
    "openai_embeddings1 = {model: get_embedding(businessNames1[0], model) for model in openai_models}\n",
    "\n",
    "for i, name2 in enumerate(businessNames2):\n",
    "    result = {\n",
    "        'Business Name 1': businessNames1[0],\n",
    "        'Business Name 2': name2,\n",
    "        'Levenshtein': levenshtein_distance(businessNames1[0], name2),\n",
    "        'N-gram': n_gram_similarity(businessNames1[0], name2),\n",
    "        'Jaro-Winkler': jaro_winkler_similarity(businessNames1[0], name2)\n",
    "    }\n",
    "    \n",
    "    for model_name, model_path in models.items():\n",
    "        model = SentenceTransformer(model_path, device=device)\n",
    "        similarities = evaluate_model_on_dataset(model, businessNames1, [name2])\n",
    "        result[f'{model_name}_Pre-fine-tuned'] = similarities[0]\n",
    "    \n",
    "    # Calculate OpenAI embeddings similarities\n",
    "    for openai_model in openai_models:\n",
    "        embedding2 = get_embedding(name2, openai_model)\n",
    "        similarity = cosine_similarity([openai_embeddings1[openai_model]], [embedding2])[0][0]\n",
    "        result[f'OpenAI_{openai_model}'] = similarity\n",
    "    \n",
    "    results.append(result)\n",
    "\n",
    "# Create DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Reorder columns\n",
    "columns = ['Business Name 1', 'Business Name 2', 'Levenshtein', 'N-gram', 'Jaro-Winkler']\n",
    "columns.extend([f'{model}_Pre-fine-tuned' for model in models])\n",
    "columns.extend([f'OpenAI_{model}' for model in openai_models])\n",
    "\n",
    "df_results = df_results[columns]\n",
    "\n",
    "print(df_results)\n",
    "\n",
    "# Optionally, save to CSV\n",
    "df_results.to_csv('business_name_similarities.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d02b092b-70c0-40f1-bc57-94c5ae94aef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Business Name 1</th>\n",
       "      <th>Business Name 2</th>\n",
       "      <th>Levenshtein</th>\n",
       "      <th>N-gram</th>\n",
       "      <th>Jaro-Winkler</th>\n",
       "      <th>SBERT_Pre-fine-tuned</th>\n",
       "      <th>RoBERTa_Pre-fine-tuned</th>\n",
       "      <th>Mini_Pre-fine-tuned</th>\n",
       "      <th>OpenAI_text-embedding-3-small</th>\n",
       "      <th>OpenAI_text-embedding-3-large</th>\n",
       "      <th>OpenAI_text-embedding-ada-002</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HANAN ATHER TRUCKING</td>\n",
       "      <td>HANAN TAHER TRUCKING</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.911981</td>\n",
       "      <td>0.913551</td>\n",
       "      <td>0.929135</td>\n",
       "      <td>0.888799</td>\n",
       "      <td>0.898423</td>\n",
       "      <td>0.968436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HANAN ATHER TRUCKING</td>\n",
       "      <td>TRUCKING INC HANAN ATHER</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.618254</td>\n",
       "      <td>0.930841</td>\n",
       "      <td>0.876942</td>\n",
       "      <td>0.875216</td>\n",
       "      <td>0.888552</td>\n",
       "      <td>0.896059</td>\n",
       "      <td>0.963726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HANAN ATHER TRUCKING</td>\n",
       "      <td>ATHER TRUCKING INC</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.729630</td>\n",
       "      <td>0.725133</td>\n",
       "      <td>0.701722</td>\n",
       "      <td>0.642829</td>\n",
       "      <td>0.752530</td>\n",
       "      <td>0.671729</td>\n",
       "      <td>0.877687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HANAN ATHER TRUCKING</td>\n",
       "      <td>GODBOUT TRUCKING INC</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.548485</td>\n",
       "      <td>0.587563</td>\n",
       "      <td>0.523199</td>\n",
       "      <td>0.465248</td>\n",
       "      <td>0.570155</td>\n",
       "      <td>0.539397</td>\n",
       "      <td>0.860756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HANAN ATHER TRUCKING</td>\n",
       "      <td>HANAN ATHER PHARMACY INC</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.590314</td>\n",
       "      <td>0.497729</td>\n",
       "      <td>0.561207</td>\n",
       "      <td>0.693619</td>\n",
       "      <td>0.698483</td>\n",
       "      <td>0.904037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HANAN ATHER TRUCKING</td>\n",
       "      <td>Ather INC</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.637963</td>\n",
       "      <td>0.483883</td>\n",
       "      <td>0.419634</td>\n",
       "      <td>0.357233</td>\n",
       "      <td>0.350427</td>\n",
       "      <td>0.384743</td>\n",
       "      <td>0.815603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Business Name 1           Business Name 2  Levenshtein    N-gram  \\\n",
       "0  HANAN ATHER TRUCKING      HANAN TAHER TRUCKING     0.900000  0.636364   \n",
       "1  HANAN ATHER TRUCKING  TRUCKING INC HANAN ATHER     0.083333  0.600000   \n",
       "2  HANAN ATHER TRUCKING        ATHER TRUCKING INC     0.500000  0.545455   \n",
       "3  HANAN ATHER TRUCKING      GODBOUT TRUCKING INC     0.300000  0.241379   \n",
       "4  HANAN ATHER TRUCKING  HANAN ATHER PHARMACY INC     0.666667  0.333333   \n",
       "5  HANAN ATHER TRUCKING                 Ather INC     0.400000  0.190476   \n",
       "\n",
       "   Jaro-Winkler  SBERT_Pre-fine-tuned  RoBERTa_Pre-fine-tuned  \\\n",
       "0      0.990000              0.911981                0.913551   \n",
       "1      0.618254              0.930841                0.876942   \n",
       "2      0.729630              0.725133                0.701722   \n",
       "3      0.548485              0.587563                0.523199   \n",
       "4      0.893333              0.590314                0.497729   \n",
       "5      0.637963              0.483883                0.419634   \n",
       "\n",
       "   Mini_Pre-fine-tuned  OpenAI_text-embedding-3-small  \\\n",
       "0             0.929135                       0.888799   \n",
       "1             0.875216                       0.888552   \n",
       "2             0.642829                       0.752530   \n",
       "3             0.465248                       0.570155   \n",
       "4             0.561207                       0.693619   \n",
       "5             0.357233                       0.350427   \n",
       "\n",
       "   OpenAI_text-embedding-3-large  OpenAI_text-embedding-ada-002  \n",
       "0                       0.898423                       0.968436  \n",
       "1                       0.896059                       0.963726  \n",
       "2                       0.671729                       0.877687  \n",
       "3                       0.539397                       0.860756  \n",
       "4                       0.698483                       0.904037  \n",
       "5                       0.384743                       0.815603  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799f7a06-f474-4d7b-bbd3-ba935d1a06a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
