{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c729f3-2975-4a61-b930-b8cf65bf48e7",
   "metadata": {},
   "source": [
    "### Evaluate traditional methods:\n",
    "\n",
    "1. Implement and test Levenshtein distance, N-gram similarity, and Jaro-Winkler similarity on a diverse dataset of business name pairs.\n",
    "Analyze their performance, strengths, and weaknesses.\n",
    "\n",
    "\n",
    "2. Explore modern NLP approaches:\n",
    "\n",
    "- Utilize pre-trained transformer models (e.g., BERT, RoBERTa) for semantic similarity matching.\n",
    "- Investigate OpenAI's embedding models for business name representation.\n",
    "\n",
    "\n",
    "3. Leverage LLM reasoning:\n",
    "\n",
    "- Design prompts for LLMs (like GPT-3 or GPT-4) to perform business name matching.\n",
    "- Explore few-shot learning techniques to improve performance.\n",
    "- Analyze the LLM's ability to handle complex cases that traditional methods struggle with.\n",
    "\n",
    "\n",
    "4. Comparative analysis:\n",
    "\n",
    "- Create a comprehensive evaluation framework to compare all methods fairly.\n",
    "- Assess performance metrics such as accuracy, precision, recall, and F1-score.\n",
    "- Analyze computational efficiency and scalability of each approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca4351fb-d474-4c72-91a2-1083688bc974",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_pairs = [\n",
    "    (\"Apple Inc.\", \"Apple Incorporated\"),\n",
    "    (\"Microsoft Corporation\", \"Microsoft Corp.\"),\n",
    "    (\"International Business Machines\", \"IBM\"),\n",
    "    (\"McDonald's\", \"McDonalds\"),\n",
    "    (\"Walmart Inc.\", \"Wal-Mart Stores Inc\"),\n",
    "    (\"Amazon.com, Inc.\", \"Amazon\"),\n",
    "    (\"The Coca-Cola Company\", \"Coca Cola Co\"),\n",
    "    (\"Johnson & Johnson\", \"Johnson and Johnson\"),\n",
    "    (\"Procter & Gamble\", \"P&G Company\"),\n",
    "    (\"General Electric Company\", \"GE\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6006c7b4-cc1b-491f-b31a-76335a49c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance as levenshtein_distance\n",
    "from nltk.util import ngrams\n",
    "from jellyfish import jaro_winkler_similarity\n",
    "\n",
    "def levenshtein_similarity(s1, s2):\n",
    "    return 1 - levenshtein_distance(s1, s2) / max(len(s1), len(s2))\n",
    "\n",
    "def ngram_similarity(s1, s2, n=2):\n",
    "    s1_ngrams = set(''.join(ng) for ng in ngrams(s1, n))\n",
    "    s2_ngrams = set(''.join(ng) for ng in ngrams(s2, n))\n",
    "    return len(s1_ngrams.intersection(s2_ngrams)) / len(s1_ngrams.union(s2_ngrams))\n",
    "\n",
    "# Jaro-Winkler similarity is already implemented in the jellyfish library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13f3f9cb-ba4e-47e9-b894-74607461a2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "levenshtein_similarity: Apple Inc. vs Apple Incorporated = 0.5000\n",
      "levenshtein_similarity: Microsoft Corporation vs Microsoft Corp. = 0.6667\n",
      "levenshtein_similarity: International Business Machines vs IBM = 0.0968\n",
      "levenshtein_similarity: McDonald's vs McDonalds = 0.9000\n",
      "levenshtein_similarity: Walmart Inc. vs Wal-Mart Stores Inc = 0.5263\n",
      "levenshtein_similarity: Amazon.com, Inc. vs Amazon = 0.3750\n",
      "levenshtein_similarity: The Coca-Cola Company vs Coca Cola Co = 0.5238\n",
      "levenshtein_similarity: Johnson & Johnson vs Johnson and Johnson = 0.8421\n",
      "levenshtein_similarity: Procter & Gamble vs P&G Company = 0.1250\n",
      "levenshtein_similarity: General Electric Company vs GE = 0.0833\n",
      "ngram_similarity: Apple Inc. vs Apple Incorporated = 0.4706\n",
      "ngram_similarity: Microsoft Corporation vs Microsoft Corp. = 0.6500\n",
      "ngram_similarity: International Business Machines vs IBM = 0.0000\n",
      "ngram_similarity: McDonald's vs McDonalds = 0.7000\n",
      "ngram_similarity: Walmart Inc. vs Wal-Mart Stores Inc = 0.4500\n",
      "ngram_similarity: Amazon.com, Inc. vs Amazon = 0.3333\n",
      "ngram_similarity: The Coca-Cola Company vs Coca Cola Co = 0.4118\n",
      "ngram_similarity: Johnson & Johnson vs Johnson and Johnson = 0.5714\n",
      "ngram_similarity: Procter & Gamble vs P&G Company = 0.0000\n",
      "ngram_similarity: General Electric Company vs GE = 0.0435\n",
      "jaro_winkler_similarity: Apple Inc. vs Apple Incorporated = 0.8800\n",
      "jaro_winkler_similarity: Microsoft Corporation vs Microsoft Corp. = 0.9200\n",
      "jaro_winkler_similarity: International Business Machines vs IBM = 0.5771\n",
      "jaro_winkler_similarity: McDonald's vs McDonalds = 0.9800\n",
      "jaro_winkler_similarity: Walmart Inc. vs Wal-Mart Stores Inc = 0.8823\n",
      "jaro_winkler_similarity: Amazon.com, Inc. vs Amazon = 0.8750\n",
      "jaro_winkler_similarity: The Coca-Cola Company vs Coca Cola Co = 0.8016\n",
      "jaro_winkler_similarity: Johnson & Johnson vs Johnson and Johnson = 0.9067\n",
      "jaro_winkler_similarity: Procter & Gamble vs P&G Company = 0.5484\n",
      "jaro_winkler_similarity: General Electric Company vs GE = 0.6944\n"
     ]
    }
   ],
   "source": [
    "def evaluate_methods(pairs):\n",
    "    results = []\n",
    "    for method in [levenshtein_similarity, ngram_similarity, jaro_winkler_similarity]:\n",
    "        method_name = method.__name__\n",
    "        for pair in pairs:\n",
    "            similarity = method(pair[0].lower(), pair[1].lower())\n",
    "            results.append((method_name, pair[0], pair[1], similarity))\n",
    "    return results\n",
    "\n",
    "evaluation_results = evaluate_methods(business_pairs)\n",
    "\n",
    "# Print results\n",
    "for method, name1, name2, similarity in evaluation_results:\n",
    "    print(f\"{method}: {name1} vs {name2} = {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a56c2ac-5bf1-477a-aefe-dd93e9b26d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hananather/Desktop/AutoText/venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Similarity: Apple Inc. vs Apple Incorporated = 0.8850\n",
      "Embedding Similarity: Microsoft Corporation vs Microsoft Corp. = 0.9297\n",
      "Embedding Similarity: International Business Machines vs IBM = 0.5283\n",
      "Embedding Similarity: McDonald's vs McDonalds = 0.9524\n",
      "Embedding Similarity: Walmart Inc. vs Wal-Mart Stores Inc = 0.9239\n",
      "Embedding Similarity: Amazon.com, Inc. vs Amazon = 0.8189\n",
      "Embedding Similarity: The Coca-Cola Company vs Coca Cola Co = 0.8977\n",
      "Embedding Similarity: Johnson & Johnson vs Johnson and Johnson = 0.9458\n",
      "Embedding Similarity: Procter & Gamble vs P&G Company = 0.4062\n",
      "Embedding Similarity: General Electric Company vs GE = 0.4141\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def embedding_similarity(s1, s2):\n",
    "    # Generate embeddings\n",
    "    embeddings = model.encode([s1, s2])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "# Function to evaluate the method\n",
    "def evaluate_embedding_method(pairs):\n",
    "    results = []\n",
    "    for pair in pairs:\n",
    "        similarity = embedding_similarity(pair[0], pair[1])\n",
    "        results.append((\"Embedding Similarity\", pair[0], pair[1], similarity))\n",
    "    return results\n",
    "\n",
    "# Evaluate the method\n",
    "embedding_results = evaluate_embedding_method(business_pairs)\n",
    "\n",
    "# Print results\n",
    "for method, name1, name2, similarity in embedding_results:\n",
    "    print(f\"{method}: {name1} vs {name2} = {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "708f7abc-2c6c-4652-abc9-74d200169841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd6cdaca5c846d899df42815e7e5962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Classification: Apple Inc. vs Apple Incorporated = Match\n",
      "Transformer Classification: Microsoft Corporation vs Microsoft Corp. = Match\n",
      "Transformer Classification: International Business Machines vs IBM = Match\n",
      "Transformer Classification: McDonald's vs McDonalds = Match\n",
      "Transformer Classification: Walmart Inc. vs Wal-Mart Stores Inc = Match\n",
      "Transformer Classification: Amazon.com, Inc. vs Amazon = Match\n",
      "Transformer Classification: The Coca-Cola Company vs Coca Cola Co = Match\n",
      "Transformer Classification: Johnson & Johnson vs Johnson and Johnson = Match\n",
      "Transformer Classification: Procter & Gamble vs P&G Company = Match\n",
      "Transformer Classification: General Electric Company vs GE = Match\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "def transformer_classification(s1, s2):\n",
    "    # Prepare input\n",
    "    inputs = tokenizer(s1, s2, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Get model output\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get prediction (0 for not match, 1 for match)\n",
    "    prediction = outputs.logits.argmax().item()\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Function to evaluate the method\n",
    "def evaluate_transformer_method(pairs):\n",
    "    results = []\n",
    "    for pair in pairs:\n",
    "        prediction = transformer_classification(pair[0], pair[1])\n",
    "        results.append((\"Transformer Classification\", pair[0], pair[1], prediction))\n",
    "    return results\n",
    "\n",
    "# Evaluate the method\n",
    "transformer_results = evaluate_transformer_method(business_pairs)\n",
    "\n",
    "# Print results\n",
    "for method, name1, name2, prediction in transformer_results:\n",
    "    print(f\"{method}: {name1} vs {name2} = {'Match' if prediction == 1 else 'No Match'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dafbd9-81a3-41ec-80dd-ae37f70e2b3c",
   "metadata": {},
   "source": [
    "Let's explore how we can leverage Large Language Models (LLMs) like GPT-3 or GPT-4 for business name matching. LLMs offer powerful reasoning capabilities that can potentially handle complex matching scenarios more effectively than traditional or even modern embedding-based methods.\n",
    "Here's how we can approach using LLMs for this task:\n",
    "\n",
    "Prompt Engineering:\n",
    "We'll need to design effective prompts that instruct the LLM to perform business name matching. Here's an example prompt:\n",
    "\n",
    "You are an expert system for matching business names. Given two business names, your task is to determine if they refer to the same company. Consider variations in spelling, abbreviations, legal suffixes, and word order. Respond with 'Match' if the names likely refer to the same company, or 'No Match' if they likely refer to different companies. Also provide a brief explanation for your decision.\n",
    "\n",
    "Business Name 1: {name1}\n",
    "Business Name 2: {name2}\n",
    "\n",
    "Are these names a match?\n",
    "\n",
    "**API Integration:**\n",
    "We'll need to integrate with an LLM API. For this example, let's assume we're using OpenAI's GPT-3.5 or GPT-4 API. Here's a Python function to interact with the API:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71832043-bfec-4a3f-acb7-b625a03bc35c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (345355743.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    You are an expert system for matching business names. Given two business names, your task is to determine if they refer to the same company. Consider variations in spelling, abbreviations, legal suffixes, and word order. Respond with 'Match' if the names likely refer to the same company, or 'No Match' if they likely refer to different companies. Also provide a brief explanation for your decision.\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef3591ed-485c-49f3-95a7-fa62de73f766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match\n",
      "\n",
      "Explanation: The two business names are a match as they refer to the same company, Apple Inc., even though one uses \"Inc.\" and the other uses \"Incorporated\". The main identifying word \"Apple\" remains the same.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def llm_match(name1, name2):\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert system for matching business names. Given two business names, your task is to determine if they refer to the same company. Consider variations in spelling, abbreviations, legal suffixes, and word order. Respond with 'Match' if the names likely refer to the same company, or 'No Match' if they likely refer to different companies. Also provide a brief explanation for your decision.\n",
    "\n",
    "    Business Name 1: {name1}\n",
    "    Business Name 2: {name2}\n",
    "\n",
    "    Are these names a match?\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  # or \"gpt-4\" if available\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in business name matching.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage\n",
    "try:\n",
    "    result = llm_match(\"Apple Inc.\", \"Apple Incorporated\")\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b74d38d-91ce-48d4-ab72-7ec58f433618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Name 1                   Name 2  Levenshtein   N-gram  Jaro-Winkler  Embedding  LLM Score   LLM Category\n",
      "HANAN ATHER TRUCKING     HANAN TAHER TRUCKING     0.900000 0.750000      0.990000   0.929135        0.8   Likely Match\n",
      "HANAN ATHER TRUCKING TRUCKING INC HANAN ATHER     0.083333 0.695652      0.618254   0.875217        1.0   Likely Match\n",
      "HANAN ATHER TRUCKING       ATHER TRUCKING INC     0.500000 0.619048      0.729630   0.642829        0.7   Likely Match\n",
      "HANAN ATHER TRUCKING     GODBOUT TRUCKING INC     0.300000 0.285714      0.548485   0.465248        0.2 Unlikely Match\n",
      "HANAN ATHER TRUCKING HANAN ATHER PHARMACY INC     0.666667 0.392857      0.893333   0.561208        0.0 Unlikely Match\n",
      "HANAN ATHER TRUCKING                Ather INC     0.400000 0.300000      0.637963   0.357234        0.2 Unlikely Match\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from nltk.util import ngrams\n",
    "from jellyfish import jaro_winkler_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# 1. Set up the dataset\n",
    "businessNames1 = [\"HANAN ATHER TRUCKING\"]\n",
    "businessNames2 = [\n",
    "    \"HANAN TAHER TRUCKING\",\n",
    "    \"TRUCKING INC HANAN ATHER\",\n",
    "    \"ATHER TRUCKING INC\",\n",
    "    \"GODBOUT TRUCKING INC\",\n",
    "    \"HANAN ATHER PHARMACY INC\",\n",
    "    \"Ather INC\"\n",
    "]\n",
    "\n",
    "# 2. Implement traditional methods\n",
    "def levenshtein_similarity(s1, s2):\n",
    "    return 1 - levenshtein_distance(s1.lower(), s2.lower()) / max(len(s1), len(s2))\n",
    "\n",
    "def ngram_similarity(s1, s2, n=2):\n",
    "    s1_ngrams = set(''.join(ng) for ng in ngrams(s1.lower(), n))\n",
    "    s2_ngrams = set(''.join(ng) for ng in ngrams(s2.lower(), n))\n",
    "    return len(s1_ngrams.intersection(s2_ngrams)) / len(s1_ngrams.union(s2_ngrams))\n",
    "\n",
    "# Jaro-Winkler similarity is already implemented in the jellyfish library\n",
    "\n",
    "# 3. Implement modern NLP method\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def embedding_similarity(s1, s2):\n",
    "    embeddings = model.encode([s1, s2])\n",
    "    return cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "\n",
    "# 4. Implement LLM-based method\n",
    "client = OpenAI()\n",
    "\n",
    "def llm_match_with_score(name1, name2):\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert system for matching business names. Given two business names, determine if they refer to the same company. Consider variations in spelling, abbreviations, legal suffixes, and word order.\n",
    "\n",
    "    Respond with a confidence score between 0 and 1, where:\n",
    "    0 means definitely not a match\n",
    "    1 means definitely a match\n",
    "\n",
    "    Provide your response in this format:\n",
    "    Score: [Your score between 0 and 1]\n",
    "    Explanation: [Brief explanation for your score]\n",
    "\n",
    "    Business Name 1: {name1}\n",
    "    Business Name 2: {name2}\n",
    "\n",
    "    What is your confidence score for these names matching?\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in business name matching.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        score_line = content.split('\\n')[0]\n",
    "        score = float(score_line.split(':')[1].strip())\n",
    "        return score\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def llm_match_with_category(name1, name2):\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert system for matching business names. Given two business names, determine if they refer to the same company. Consider variations in spelling, abbreviations, legal suffixes, and word order.\n",
    "\n",
    "    Categorize your confidence in the match using one of these categories:\n",
    "    - Definite Match\n",
    "    - Likely Match\n",
    "    - Possible Match\n",
    "    - Unlikely Match\n",
    "    - Definite Non-Match\n",
    "\n",
    "    Provide your response in this format:\n",
    "    Category: [Your chosen category]\n",
    "    Explanation: [Brief explanation for your category choice]\n",
    "\n",
    "    Business Name 1: {name1}\n",
    "    Business Name 2: {name2}\n",
    "\n",
    "    What is your confidence category for these names matching?\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in business name matching.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        category_line = content.split('\\n')[0]\n",
    "        category = category_line.split(':')[1].strip()\n",
    "        return category\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Update the compare_methods function\n",
    "def compare_methods(name1, name2):\n",
    "    return {\n",
    "        'Levenshtein': levenshtein_similarity(name1, name2),\n",
    "        'N-gram': ngram_similarity(name1, name2),\n",
    "        'Jaro-Winkler': jaro_winkler_similarity(name1.lower(), name2.lower()),\n",
    "        'Embedding': embedding_similarity(name1, name2),\n",
    "        'LLM Score': llm_match_with_score(name1, name2),\n",
    "        'LLM Category': llm_match_with_category(name1, name2)\n",
    "    }\n",
    "\n",
    "# Run comparisons and create a dataframe\n",
    "results = []\n",
    "for name2 in businessNames2:\n",
    "    result = compare_methods(businessNames1[0], name2)\n",
    "    result['Name 1'] = businessNames1[0]\n",
    "    result['Name 2'] = name2\n",
    "    results.append(result)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Reorder columns\n",
    "columns_order = ['Name 1', 'Name 2', 'Levenshtein', 'N-gram', 'Jaro-Winkler', 'Embedding', 'LLM Score', 'LLM Category']\n",
    "df = df[columns_order]\n",
    "\n",
    "# Display the results\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ba6695-01d9-4709-a982-fdc0bf7f2f73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
