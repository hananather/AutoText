{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f168579c-e0f6-4b98-8c7c-063ee4013232",
   "metadata": {},
   "source": [
    "Pre-trained models like BERT are initially trained on a large corpus of text (e.g., Wikipedia, BookCorpus) to learn general language representations. **Transfer learning** involves fine-tuning these pre-trained models on specific task, leveraging pre-exisitng knowledge to achieve better performance with less data and training time. \n",
    "\n",
    "\n",
    "## Understanding BERT's Architecture\n",
    "- Bidirectional: considers context from both left and right of a token\n",
    "- Transformer Encoder: Utilizes self-attention mechanism to build representations of words based on their context.\n",
    "\n",
    "## Task-Specific Fine-Tuning For Similarity Detection\n",
    "Each sample in the dataset consists of a pair of business names and a label indicating their similarity (0 for dissimilar,1 for similar). \n",
    "**Tokenization:** Tokenize the pairs of names using BERT's tokenizer. This includes adding special token ([CLS], [SEP]) to distingguish separate names\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989ac560-840b-4252-a875-3f65dfdba874",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(name1, name2, return_tensor = 'pt', padding = 'max_length', truncation = True,  max_length = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1d6a4d-4c6e-453f-9507-e29afd8983bb",
   "metadata": {},
   "source": [
    "- We use the pre-trained BERT model to encode the input pairs. \n",
    "- Aggregate the token embeddings (e.g., mean pooling) to obtain a fixed-size representation. \n",
    "- Add a linear layer to map the pooled representation to similarity score.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5cf466-5bc9-49bb-9587-c5c9dc1684df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91464aa0-3b8a-45b1-8e94-d0193b41f76d",
   "metadata": {},
   "source": [
    "**Loss function and Optimization**: Use a suitable loss function for binary classification (e.g., Binary Cross-Entropy Loss).\n",
    "\n",
    "**Training Loop**:\n",
    "- Forward pass: compute the similarity score for input pairs\n",
    "- Loss computation: Calculate the loss between predicted similarity scores and actual labels. \n",
    "- Backward Pass and Optimization: Update model parameters based on the gradient.\n",
    "\n",
    "### Why Tokenize Pairs Together?\n",
    "\n",
    "**Contextual Relationship**:\n",
    "By tokenizing pairs together and feeding them into the model, BERT can consider the interation between the two names in a single forward pass. This allows the model to learn more about their relationship, which is crucial for taks like similarity detection. \n",
    "When embeddings are generated separately, the model cannot leverage the full context of both names together. Comparing separate embeddings via cosine similarity captures some relational informatio but lacks the deeper interaction modeling that joint tokenization provides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73c964e-e46a-4e82-acbc-c4789977b0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
