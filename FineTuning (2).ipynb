{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f168579c-e0f6-4b98-8c7c-063ee4013232",
   "metadata": {},
   "source": [
    "Pre-trained models like BERT are initially trained on a large corpus of text (e.g., Wikipedia, BookCorpus) to learn general language representations. **Transfer learning** involves fine-tuning these pre-trained models on specific task, leveraging pre-exisitng knowledge to achieve better performance with less data and training time. \n",
    "\n",
    "\n",
    "## Understanding BERT's Architecture\n",
    "- Bidirectional: considers context from both left and right of a token\n",
    "- Transformer Encoder: Utilizes self-attention mechanism to build representations of words based on their context.\n",
    "\n",
    "## Task-Specific Fine-Tuning For Similarity Detection\n",
    "Each sample in the dataset consists of a pair of business names and a label indicating their similarity (0 for dissimilar,1 for similar). \n",
    "**Tokenization:** Tokenize the pairs of names using BERT's tokenizer. This includes adding special token ([CLS], [SEP]) to distingguish separate names\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989ac560-840b-4252-a875-3f65dfdba874",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(name1, name2, return_tensor = 'pt', padding = 'max_length', truncation = True,  max_length = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1d6a4d-4c6e-453f-9507-e29afd8983bb",
   "metadata": {},
   "source": [
    "- We use the pre-trained BERT model to encode the input pairs. \n",
    "- Aggregate the token embeddings (e.g., mean pooling) to obtain a fixed-size representation. \n",
    "- Add a linear layer to map the pooled representation to similarity score.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5cf466-5bc9-49bb-9587-c5c9dc1684df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91464aa0-3b8a-45b1-8e94-d0193b41f76d",
   "metadata": {},
   "source": [
    "**Loss function and Optimization**: Use a suitable loss function for binary classification (e.g., Binary Cross-Entropy Loss).\n",
    "\n",
    "**Training Loop**:\n",
    "- Forward pass: compute the similarity score for input pairs\n",
    "- Loss computation: Calculate the loss between predicted similarity scores and actual labels. \n",
    "- Backward Pass and Optimization: Update model parameters based on the gradient.\n",
    "\n",
    "### Why Tokenize Pairs Together?\n",
    "\n",
    "**Contextual Relationship**:\n",
    "By tokenizing pairs together and feeding them into the model, BERT can consider the interation between the two names in a single forward pass. This allows the model to learn more about their relationship, which is crucial for taks like similarity detection. \n",
    "When embeddings are generated separately, the model cannot leverage the full context of both names together. Comparing separate embeddings via cosine similarity captures some relational informatio but lacks the deeper interaction m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce1ebb6-5295-47eb-adf0-302a4fabe0f2",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "- BERT Paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
    "- Transfer Learning in NLP: Transfer Learning in NLP\n",
    "- Attention Is All You Need: Attention Is All You Need\n",
    "- Sentence-BERT: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\n",
    "- The Illustrated Transformer: The Illustrated Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d295785-018f-43fe-a852-f47760107e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "from nltk.util import ngrams\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494636aa-1a26-491e-9d93-fd1e5abeebca",
   "metadata": {},
   "source": [
    "### Business Names Dataset Class\n",
    "The `dataset` class loads the business name pairs from CSV file, tokenizes the text, prepares the input for the model:\n",
    "\n",
    "Questions: \n",
    "- what is the purpose of max length?\n",
    "- what are input_ids (inputs to the BERT)\n",
    "- what are attention masks? (Input to BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f8cb5fd-0f33-4385-9447-85f7a767824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BusinessNamesDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=128):\n",
    "        self.data = pd.read_csv(file_path)  # Load data from CSV\n",
    "        self.tokenizer = tokenizer  # BERT tokenizer\n",
    "        self.max_length = max_length  # Maximum sequence length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # Number of samples in the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name1 = self.data.iloc[idx]['name1']  # First business name\n",
    "        name2 = self.data.iloc[idx]['name2']  # Second business name\n",
    "        label = self.data.iloc[idx]['label']  # Similarity label (0 or 1)\n",
    "        inputs = self.tokenizer(name1, name2, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
    "        input_ids = inputs['input_ids'].squeeze(0)  # Token IDs\n",
    "        attention_mask = inputs['attention_mask'].squeeze(0)  # Attention mask\n",
    "        return input_ids, attention_mask, torch.tensor(label, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb531098-bf43-45fb-a9e8-d6b4afee21a4",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "The model class defines the architecture for fine-tuning BERT to compute similarity scores.\n",
    "We add a **linear layer** to map the pooled BERT embeddings to a similarity score.\n",
    "- **BERT Forward:** Passes the input through BERT to obtain the hidden states.\n",
    "- **Pooling:** Averages the hidden states to get a single vector representing the input pair\n",
    "- **Similarity score:** Computes the similarity score using the linear layer\n",
    "\n",
    "Questions/Concerns:\n",
    "- `hidden states` of BERT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d87b4be-c772-4306-801b-47bdbe050f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BusinessNamesModel(torch.nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(BusinessNamesModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name) #load pretrained BERT\n",
    "        self.similarity = torch.nn.Linear(self.bert.config.hiddensize,1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask = attention_mask) # Forward pass through BERT\n",
    "        pooled_out = outputs.last_hidden_state.mean(dim=1) # Mean pooling\n",
    "        similarity_score = self.similarity(pooled_output)\n",
    "        return similarity_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6629f7a-8a05-46d5-bc44-4cc88963dc09",
   "metadata": {},
   "source": [
    "## Training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5df3c165-cacc-488b-a016-5d1c4b04dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader= None, num_epochs=3, learning_rate=2e-5):\n",
    "    criterion = torch.nnBCEWithLogitsLoss() # loss funciton\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate) # Optimizer\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\") # device  configuration\n",
    "    model.to(device) # Move model to device\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad() # Reset gradients\n",
    "            outputs = model(input_ids, attention_mask) # Forward pass\n",
    "            loss = criterion(outputs.squeeze(-1), labels.float()) # Compute loss\n",
    "            loss.backward() # Backpropagation\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {tota_loss/len(train_dataloader)}')\n",
    "\n",
    "        # Validation\n",
    "        if val_dataloader:\n",
    "            model.eval()\n",
    "            val_loss = 0 \n",
    "            with torch.no_grad():\n",
    "                for batch in val_dataloader:\n",
    "                    input_ids, attention_mask, labels = batch\n",
    "                    input_ids = input_ids.to(device)\n",
    "                    attention_mask = attention_mask.to(device)\n",
    "                    labels = labels.to(devince)\n",
    "\n",
    "\n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    loss = criterion(outputs.squeeze(-1), labels.float())\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Validation Loss: {val_loss/len(val_dataloader)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61274ed8-4146-46ff-99cb-eb4646eebf11",
   "metadata": {},
   "source": [
    "## Using the Fine-Tuned Model\n",
    "\n",
    "This class uses the fine-tuned model to obtain embeddings for new business names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89965545-6143-4804-9ae5-e5d2498c93f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTunedEmbedding:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenzier\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensor = 'pt', padding = True, truncation = True)\n",
    "        inputs = {k: v.to(self.devince) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.bert(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f81eac-7c56-4fdc-9e79-53ff0ca12c43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
