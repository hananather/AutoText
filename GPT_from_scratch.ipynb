{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBubOeJbui1snG61f31KSa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hananather/AutoText/blob/main/GPT_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYNLbCpol9pl",
        "outputId": "66cf1a7b-a322-4a1c-cce7-28d7647974b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-12 14:37:04--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-08-12 14:37:04 (18.0 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "9SeJw9XfmvkH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQvjDNR6nKr9",
        "outputId": "70cca70a-1da8-43e3-9919-8b7389d52cc2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I0BKlzoolgB",
        "outputId": "4611df54-e469-4e8e-a97c-916555bc0cba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our objective is to tokenize each character to an integer."
      ],
      "metadata": {
        "id": "h0SePIQFBkel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(chars)\n",
        "stoi = {s: i for i,s in enumerate(chars)}\n",
        "itos = {i:s for i,s in enumerate(chars)}"
      ],
      "metadata": {
        "id": "RTPFfojhopfM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f355fb7a-cba2-4b47-cb46-7ac16ca3b0ee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])"
      ],
      "metadata": {
        "id": "HOGgSj1eyxKh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode(\"Hanan\"))\n",
        "print(decode(encode(\"Hanan\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ8WyXdQz-9x",
        "outputId": "c8c71c4e-5def-4325-c024-594a4d1b223e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20, 39, 52, 39, 52]\n",
            "Hanan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the entire dataset and store it into torch.Tensor\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data.shape,  data.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF7InqWx0FFH",
        "outputId": "bafede89-e616-4144-9b54-6679ace18125"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into train and validation split\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "kHKeNKmR0ZiA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size +1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1e1z3h11lZz",
        "outputId": "fb16e34c-47fe-463e-c70d-47174a2fab1d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size + 1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"input is: {context} targer is: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dKNo6SU1uru",
        "outputId": "1a8ce206-a0eb-430f-97e9-66987f896e8e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input is: tensor([18]) targer is: 47\n",
            "input is: tensor([18, 47]) targer is: 56\n",
            "input is: tensor([18, 47, 56]) targer is: 57\n",
            "input is: tensor([18, 47, 56, 57]) targer is: 58\n",
            "input is: tensor([18, 47, 56, 57, 58]) targer is: 1\n",
            "input is: tensor([18, 47, 56, 57, 58,  1]) targer is: 15\n",
            "input is: tensor([18, 47, 56, 57, 58,  1, 15]) targer is: 47\n",
            "input is: tensor([18, 47, 56, 57, 58,  1, 15, 47]) targer is: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # number of independent sequences\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "  # generate a small batch of data\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i: i +block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1: i+block_size +1] for i in ix])\n",
        "  return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "print('----------')\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b, t]\n",
        "    print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_9W2dSV2H9B",
        "outputId": "e0e4de3a-a5bf-4ee4-aec4-929fcf6a9d55"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----------\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import  torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets):\n",
        "\n",
        "    logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "    return logits\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "out = m(xb,yb)\n",
        "print(out)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "id": "15UX_rVqbhba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3196cdd6-4044-4d95-8254-f5c408281b16"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
            "         [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
            "         [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
            "         ...,\n",
            "         [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
            "         [ 1.0901,  0.2170, -2.9996,  ..., -0.5472, -0.8017,  0.7761],\n",
            "         [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594]],\n",
            "\n",
            "        [[ 1.0541,  1.5018, -0.5266,  ...,  1.8574,  1.5249,  1.3035],\n",
            "         [-0.1324, -0.5489,  0.1024,  ..., -0.8599, -1.6050, -0.6985],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 1.0901,  0.2170, -2.9996,  ..., -0.5472, -0.8017,  0.7761],\n",
            "         [ 1.1513,  1.0539,  3.4105,  ..., -0.5686,  0.9079, -0.1701],\n",
            "         [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305]],\n",
            "\n",
            "        [[-0.2103,  0.4481,  1.2381,  ...,  1.3597, -0.0821,  0.3909],\n",
            "         [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
            "         [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
            "         ...,\n",
            "         [ 1.1513,  1.0539,  3.4105,  ..., -0.5686,  0.9079, -0.1701],\n",
            "         [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
            "         [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275]],\n",
            "\n",
            "        [[ 0.0691,  0.2990, -1.4717,  ...,  0.1517,  0.8528,  0.0604],\n",
            "         [-0.4892, -2.5589,  1.4134,  ..., -1.4296,  0.2347, -1.2034],\n",
            "         [-0.1600,  1.3981, -0.7047,  ..., -1.9908,  0.8574, -2.1603],\n",
            "         ...,\n",
            "         [-2.1910, -0.7574,  1.9656,  ..., -0.3580,  0.8585, -0.6161],\n",
            "         [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
            "         [-0.6787,  0.8662, -1.6433,  ...,  2.3671, -0.7775, -0.2586]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([4, 8, 65])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedding**\n",
        "A simple lookup table that stores embeddings of fixed dictionary and size.\n",
        "\n",
        "This module is often used to store word embeddings and retrieve them using indicies. The input to the module is a list of indicies, and the output is the corresponding word embeddings.\n",
        "\n",
        "**Cross Entropy Loss**\n",
        "Computes the cross entropy loss between input logits and target.\n",
        "- the input is expected to contain the unnormalized logits for each class (which we don't need to be positie for sum to 1, in general)\n",
        "- input has to be a tensor of of size (minibatch, C)"
      ],
      "metadata": {
        "id": "QZvS-fCmIwbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import  torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets = None):\n",
        "    logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indicies in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get the pridictions\n",
        "      logits, loss = self(idx)\n",
        "      # focus only on the last time step\n",
        "      logits = logits[:,-1,:] # becomes (B,C)\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=1) # (B, C)\n",
        "      # sample from distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples = 1) #( B, 1)\n",
        "      # append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb,yb)\n",
        "print(logits.shape)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "0pAAm-NbzBO0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dce2d922-cea8-45bf-e2d2-bc981610bcc7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1,1), dtype= torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nWiEekDlz6R",
        "outputId": "0b840df1-1162-4023-e082-12d8fb8ed930"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a Pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "id": "lLtLewNtrzzl"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size  = 32\n",
        "for steps in range(10000):\n",
        "\n",
        "  # sample a batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none = True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAEmzevzvX2Q",
        "outputId": "58639164-2b1b-4eb9-a488-9075433366dc"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3997652530670166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1,1), dtype= torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbiK5ggowwdD",
        "outputId": "b0bcbb50-9d0d-46c2-aa59-6c6fcd0c7ecd"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "HUCI 'lilvit 't mig ater a lethye wde wasthod f mb.\n",
            "WA:\n",
            "\n",
            "ad shalYouers! brema'suD bems as II wit sir\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1,1), dtype= torch.long), max_new_tokens=300)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3vijHEXxXPV",
        "outputId": "05a5fabe-cf22-4364-fd7b-69a98c5e791a"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "INurugy I's heleveind.\n",
            "Walo hilleicu to athief thed blesst' pr,\n",
            "CHENo O: l,\n",
            "LOn taior dve\n",
            "lit,\n",
            "Im mothy;\n",
            "ARLALI d att unt'darisreathousefthice coexace ton hoth t ffe oithi'dito I I Ino truret\n",
            "anathopusuthig ofarid ke s wherelak, veirrfaichired t ctor s ke,\n",
            "DWh gund the;\n",
            "xceat y, at o CO:-n thuf t\n"
          ]
        }
      ]
    }
  ]
}