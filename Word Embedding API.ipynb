{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ace28d-785a-48bd-9361-fb92660a488d",
   "metadata": {},
   "source": [
    "# What are embeddings?\n",
    "- Embeddings are a fundamental concept from natural language processings; where words, phrases, or entire documents are represented in numerical form.\n",
    "- More fundamentally, embeddings models  map text onto a multi-dimensional space, or vector space, and the numbers outputted by the model represent the location of the text in that space.\n",
    "- Simillar pieces of text or words, like teacher and student, are mapped closer together in the space and dissimilar words are mapped futher away.\n",
    "- this abilitity  to map similar and dissimilar words means that embedding models can be used to capture the **semantic meaning** of text. (by semantic meaning, we mean the full context and intent of the word is captured)\n",
    "- \"which way is to the supermarkey\" vs. \"Could I have directions to the shop\" only have two words in common but semantically very similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cfd489f-3683-4197-aa13-0829e969e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    df['name1'] = df['name1'].str.lower().str.strip()\n",
    "    df['name2'] = df['name2'].str.lower().str.strip()\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a3cdc20-ce2c-4401-900b-cf5c3c58e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def levenshtein_distance(str1, str2):\n",
    "    distance = Levenshtein.distance(str1, str2)\n",
    "    max_len = max(len(str1), len(str2))\n",
    "    return distance / max_len\n",
    "\n",
    "def n_gram_similarity(str1, str2, n=3):\n",
    "    str1_ngrams = set(ngrams(str1, n))\n",
    "    str2_ngrams = set(ngrams(str2, n))\n",
    "    return len(str1_ngrams & str2_ngrams) / float(len(str1_ngrams | str2_ngrams))\n",
    "\n",
    "def jaro_winkler_similarity(str1, str2):\n",
    "    return Levenshtein.jaro_winkler(str1, str2)\n",
    "\n",
    "def tfidf_cosine_similarity(corpus):\n",
    "    vectorizer = TfidfVectorizer().fit_transform(corpus)\n",
    "    vectors = vectorizer.toarray()\n",
    "    return cosine_similarity(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f8a681e-772b-4a96-a7bb-1f93e684e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "\n",
    "class HuggingFaceEmbedding:\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\", api_key=None):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=api_key)\n",
    "        self.model = AutoModel.from_pretrained(model_name, use_auth_token=api_key)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors='pt')\n",
    "        outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "244c8d1e-f450-4131-a917-cd34ad49df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def compare_algorithms(df):\n",
    "    results = []\n",
    "\n",
    "    hf_embedding = HuggingFaceEmbedding(api_key=\"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\")\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        name1 = row['name1']\n",
    "        name2 = row['name2']\n",
    "\n",
    "        lev_dist = levenshtein_distance(name1, name2)\n",
    "        ngram_sim = n_gram_similarity(name1, name2)\n",
    "        jw_sim = jaro_winkler_similarity(name1, name2)\n",
    "\n",
    "        emb1 = hf_embedding.get_embedding(name1)\n",
    "        emb2 = hf_embedding.get_embedding(name2)\n",
    "        embedding_sim = cosine_similarity(emb1, emb2)[0, 0]\n",
    "\n",
    "        results.append({\n",
    "            \"name1\": name1,\n",
    "            \"name2\": name2,\n",
    "            \"levenshtein_distance\": lev_dist,\n",
    "            \"n_gram_similarity\": ngram_sim,\n",
    "            \"jaro_winkler_similarity\": jw_sim,\n",
    "            \"embedding_similarity\": embedding_sim\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "918be3f9-dd35-4734-bb3f-9e9a5c036a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(\"data/sample_data.csv\")\n",
    "df = preprocess_data(df)\n",
    "#results = compare_algorithms(df)\n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a478abf-d10e-43ae-a7b4-0a8b768b0ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name1</th>\n",
       "      <th>name2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple inc.</td>\n",
       "      <td>apple incorporated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google llc</td>\n",
       "      <td>googol l.l.c.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>microsoft corporation</td>\n",
       "      <td>micro soft corp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazon.com inc.</td>\n",
       "      <td>amazon incorporated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>facebook inc.</td>\n",
       "      <td>face book incorporated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>netflix, inc.</td>\n",
       "      <td>net flix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tesla, inc.</td>\n",
       "      <td>teslar inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>oracle corporation</td>\n",
       "      <td>orakel corp.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ibm</td>\n",
       "      <td>international business machines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>adobe systems inc.</td>\n",
       "      <td>adoby systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>statistics canada</td>\n",
       "      <td>statcan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>canada revenue agency</td>\n",
       "      <td>cra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>environment and climate change canada</td>\n",
       "      <td>eccc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>health canada</td>\n",
       "      <td>hc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>public services and procurement canada</td>\n",
       "      <td>pspc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>shopify inc.</td>\n",
       "      <td>shopify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>royal bank of canada</td>\n",
       "      <td>rbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>brookfield asset management</td>\n",
       "      <td>brookfield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>canadian national railway</td>\n",
       "      <td>cn railway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>thomson reuters</td>\n",
       "      <td>thomson reuters corp.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     name1                            name2\n",
       "0                               apple inc.               apple incorporated\n",
       "1                               google llc                    googol l.l.c.\n",
       "2                    microsoft corporation                  micro soft corp\n",
       "3                          amazon.com inc.              amazon incorporated\n",
       "4                            facebook inc.           face book incorporated\n",
       "5                            netflix, inc.                         net flix\n",
       "6                              tesla, inc.                      teslar inc.\n",
       "7                       oracle corporation                     orakel corp.\n",
       "8                                      ibm  international business machines\n",
       "9                       adobe systems inc.                    adoby systems\n",
       "10                       statistics canada                          statcan\n",
       "11                   canada revenue agency                              cra\n",
       "12   environment and climate change canada                             eccc\n",
       "13                           health canada                               hc\n",
       "14  public services and procurement canada                             pspc\n",
       "15                            shopify inc.                          shopify\n",
       "16                    royal bank of canada                              rbc\n",
       "17             brookfield asset management                       brookfield\n",
       "18               canadian national railway                       cn railway\n",
       "19                         thomson reuters            thomson reuters corp."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b953808-4140-4de2-8cd4-275f8b77db27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hananather/Desktop/AutoText/venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:778: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/Users/hananather/Desktop/AutoText/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     name1                            name2  \\\n",
      "0                               apple inc.               apple incorporated   \n",
      "1                               google llc                    googol l.l.c.   \n",
      "2                    microsoft corporation                  micro soft corp   \n",
      "3                          amazon.com inc.              amazon incorporated   \n",
      "4                            facebook inc.           face book incorporated   \n",
      "5                            netflix, inc.                         net flix   \n",
      "6                              tesla, inc.                      teslar inc.   \n",
      "7                       oracle corporation                     orakel corp.   \n",
      "8                                      ibm  international business machines   \n",
      "9                       adobe systems inc.                    adoby systems   \n",
      "10                       statistics canada                          statcan   \n",
      "11                   canada revenue agency                              cra   \n",
      "12   environment and climate change canada                             eccc   \n",
      "13                           health canada                               hc   \n",
      "14  public services and procurement canada                             pspc   \n",
      "15                            shopify inc.                          shopify   \n",
      "16                    royal bank of canada                              rbc   \n",
      "17             brookfield asset management                       brookfield   \n",
      "18               canadian national railway                       cn railway   \n",
      "19                         thomson reuters            thomson reuters corp.   \n",
      "\n",
      "    levenshtein_distance  n_gram_similarity  jaro_winkler_similarity  \\\n",
      "0               0.500000           0.411765                 0.880000   \n",
      "1               0.384615           0.117647                 0.918462   \n",
      "2               0.380952           0.454545                 0.891429   \n",
      "3               0.578947           0.250000                 0.826093   \n",
      "4               0.454545           0.347826                 0.860373   \n",
      "5               0.538462           0.214286                 0.851923   \n",
      "6               0.090909           0.500000                 0.963636   \n",
      "7               0.555556           0.190476                 0.834074   \n",
      "8               0.903226           0.000000                 0.577061   \n",
      "9               0.333333           0.421053                 0.884615   \n",
      "10              0.588235           0.176471                 0.882353   \n",
      "11              0.857143           0.000000                 0.603175   \n",
      "12              0.891892           0.000000                 0.518018   \n",
      "13              0.846154           0.000000                 0.525641   \n",
      "14              0.894737           0.000000                 0.618421   \n",
      "15              0.416667           0.500000                 0.916667   \n",
      "16              0.850000           0.000000                 0.588889   \n",
      "17              0.629630           0.320000                 0.874074   \n",
      "18              0.600000           0.240000                 0.564762   \n",
      "19              0.285714           0.684211                 0.942857   \n",
      "\n",
      "    embedding_similarity  \n",
      "0               0.891946  \n",
      "1               0.507944  \n",
      "2               0.812509  \n",
      "3               0.779390  \n",
      "4               0.788614  \n",
      "5               0.703022  \n",
      "6               0.905813  \n",
      "7               0.750406  \n",
      "8               0.775839  \n",
      "9               0.745221  \n",
      "10              0.749285  \n",
      "11              0.644628  \n",
      "12              0.600861  \n",
      "13              0.789979  \n",
      "14              0.613569  \n",
      "15              0.874833  \n",
      "16              0.646269  \n",
      "17              0.807272  \n",
      "18              0.880164  \n",
      "19              0.876252  \n"
     ]
    }
   ],
   "source": [
    "df = load_data(\"data/sample_data.csv\")\n",
    "df = preprocess_data(df)\n",
    "results = compare_algorithms(df)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a18db30a-d74a-498f-8de6-d62ea77f81cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Record 1</th>\n",
       "      <th>Record 2</th>\n",
       "      <th>Levenshtein Distance</th>\n",
       "      <th>N-Gram Similarity</th>\n",
       "      <th>Jaro-Winkler Similarity</th>\n",
       "      <th>Embedding Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HANAN TAHER TRUCKING</td>\n",
       "      <td>TRUCKING INC HANAN ATHER</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.618254</td>\n",
       "      <td>0.898988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HANAN TAHER TRUCKING</td>\n",
       "      <td>ATHER TRUCKING INC</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.729630</td>\n",
       "      <td>0.794868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HANAN TAHER TRUCKING</td>\n",
       "      <td>GODBOUT TRUCKING INC</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.548485</td>\n",
       "      <td>0.776235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HANAN TAHER TRUCKING</td>\n",
       "      <td>HANAN ATHER PHARMACY INC</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.880833</td>\n",
       "      <td>0.888444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HANAN TAHER TRUCKING</td>\n",
       "      <td>Ather INC</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.464815</td>\n",
       "      <td>0.682862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TRUCKING INC HANAN ATHER</td>\n",
       "      <td>ATHER TRUCKING INC</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.644180</td>\n",
       "      <td>0.893964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TRUCKING INC HANAN ATHER</td>\n",
       "      <td>GODBOUT TRUCKING INC</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.576709</td>\n",
       "      <td>0.822202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TRUCKING INC HANAN ATHER</td>\n",
       "      <td>HANAN ATHER PHARMACY INC</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.886467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TRUCKING INC HANAN ATHER</td>\n",
       "      <td>Ather INC</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.453704</td>\n",
       "      <td>0.820383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ATHER TRUCKING INC</td>\n",
       "      <td>GODBOUT TRUCKING INC</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.802116</td>\n",
       "      <td>0.919661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ATHER TRUCKING INC</td>\n",
       "      <td>HANAN ATHER PHARMACY INC</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.626425</td>\n",
       "      <td>0.847425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ATHER TRUCKING INC</td>\n",
       "      <td>Ather INC</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.544444</td>\n",
       "      <td>0.866637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GODBOUT TRUCKING INC</td>\n",
       "      <td>HANAN ATHER PHARMACY INC</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.497222</td>\n",
       "      <td>0.780057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GODBOUT TRUCKING INC</td>\n",
       "      <td>Ather INC</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.464815</td>\n",
       "      <td>0.747102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HANAN ATHER PHARMACY INC</td>\n",
       "      <td>Ather INC</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.453704</td>\n",
       "      <td>0.794609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Record 1                  Record 2  Levenshtein Distance  \\\n",
       "0       HANAN TAHER TRUCKING  TRUCKING INC HANAN ATHER              0.916667   \n",
       "1       HANAN TAHER TRUCKING        ATHER TRUCKING INC              0.500000   \n",
       "2       HANAN TAHER TRUCKING      GODBOUT TRUCKING INC              0.700000   \n",
       "3       HANAN TAHER TRUCKING  HANAN ATHER PHARMACY INC              0.416667   \n",
       "4       HANAN TAHER TRUCKING                 Ather INC              0.800000   \n",
       "5   TRUCKING INC HANAN ATHER        ATHER TRUCKING INC              0.750000   \n",
       "6   TRUCKING INC HANAN ATHER      GODBOUT TRUCKING INC              0.833333   \n",
       "7   TRUCKING INC HANAN ATHER  HANAN ATHER PHARMACY INC              0.916667   \n",
       "8   TRUCKING INC HANAN ATHER                 Ather INC              0.833333   \n",
       "9         ATHER TRUCKING INC      GODBOUT TRUCKING INC              0.350000   \n",
       "10        ATHER TRUCKING INC  HANAN ATHER PHARMACY INC              0.583333   \n",
       "11        ATHER TRUCKING INC                 Ather INC              0.722222   \n",
       "12      GODBOUT TRUCKING INC  HANAN ATHER PHARMACY INC              0.750000   \n",
       "13      GODBOUT TRUCKING INC                 Ather INC              0.800000   \n",
       "14  HANAN ATHER PHARMACY INC                 Ather INC              0.791667   \n",
       "\n",
       "    N-Gram Similarity  Jaro-Winkler Similarity  Embedding Similarity  \n",
       "0            0.379310                 0.618254              0.898988  \n",
       "1            0.416667                 0.729630              0.794868  \n",
       "2            0.241379                 0.548485              0.776235  \n",
       "3            0.176471                 0.880833              0.888444  \n",
       "4            0.000000                 0.464815              0.682862  \n",
       "5            0.520000                 0.644180              0.893964  \n",
       "6            0.333333                 0.576709              0.822202  \n",
       "7            0.333333                 0.616667              0.886467  \n",
       "8            0.074074                 0.453704              0.820383  \n",
       "9            0.478261                 0.802116              0.919661  \n",
       "10           0.187500                 0.626425              0.847425  \n",
       "11           0.095238                 0.544444              0.866637  \n",
       "12           0.052632                 0.497222              0.780057  \n",
       "13           0.086957                 0.464815              0.747102  \n",
       "14           0.074074                 0.453704              0.794609  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to compute Levenshtein distance\n",
    "def levenshtein_distance(str1, str2):\n",
    "    distance = Levenshtein.distance(str1, str2)\n",
    "    max_len = max(len(str1), len(str2))\n",
    "    return distance / max_len\n",
    "\n",
    "# Function to compute N-gram similarity\n",
    "def n_gram_similarity(str1, str2, n=3):\n",
    "    str1_ngrams = set(ngrams(str1, n))\n",
    "    str2_ngrams = set(ngrams(str2, n))\n",
    "    return len(str1_ngrams & str2_ngrams) / float(len(str1_ngrams | str2_ngrams))\n",
    "\n",
    "# Function to compute Jaro-Winkler similarity\n",
    "def jaro_winkler_similarity(str1, str2):\n",
    "    return Levenshtein.jaro_winkler(str1, str2)\n",
    "\n",
    "# Function to compute TF-IDF cosine similarity\n",
    "def tfidf_cosine_similarity(corpus):\n",
    "    vectorizer = TfidfVectorizer().fit_transform(corpus)\n",
    "    vectors = vectorizer.toarray()\n",
    "    return cosine_similarity(vectors)\n",
    "\n",
    "# HuggingFace Embedding class\n",
    "class HuggingFaceEmbedding:\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors='pt')\n",
    "        outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Define the records\n",
    "records = [\n",
    "    \"HANAN TAHER TRUCKING\",\n",
    "    \"TRUCKING INC HANAN ATHER\",\n",
    "    \"ATHER TRUCKING INC\",\n",
    "    \"GODBOUT TRUCKING INC\",\n",
    "    \"HANAN ATHER PHARMACY INC\",\n",
    "    \"Ather INC\"\n",
    "]\n",
    "\n",
    "# Compare algorithms\n",
    "def compare_algorithms(records):\n",
    "    results = []\n",
    "    hf_embedding = HuggingFaceEmbedding()\n",
    "\n",
    "    for i in range(len(records)):\n",
    "        for j in range(i + 1, len(records)):\n",
    "            name1 = records[i]\n",
    "            name2 = records[j]\n",
    "\n",
    "            lev_dist = levenshtein_distance(name1, name2)\n",
    "            ngram_sim = n_gram_similarity(name1, name2)\n",
    "            jw_sim = jaro_winkler_similarity(name1, name2)\n",
    "\n",
    "            emb1 = hf_embedding.get_embedding(name1)\n",
    "            emb2 = hf_embedding.get_embedding(name2)\n",
    "            embedding_sim = cosine_similarity(emb1, emb2)[0, 0]\n",
    "\n",
    "            results.append({\n",
    "                \"Record 1\": name1,\n",
    "                \"Record 2\": name2,\n",
    "                \"Levenshtein Distance\": lev_dist,\n",
    "                \"N-Gram Similarity\": ngram_sim,\n",
    "                \"Jaro-Winkler Similarity\": jw_sim,\n",
    "                \"Embedding Similarity\": embedding_sim\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run the comparison\n",
    "df_results = compare_algorithms(records)\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc554a10-ef55-47df-91eb-91d642580c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
