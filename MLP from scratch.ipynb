{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dca4d3e-a3ea-4be2-8ab2-8c3aa5e44abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71b9872a-5a28-4308-bc61-84b50870aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e4a2e6b-c40e-4b77-857b-d577cdf9fe95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '.': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = {s: i+1 for i, s in enumerate(sorted(list(set(''.join(words)))))}\n",
    "stoi['.'] = 0\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a910ba94-d94b-4209-9297-dc03e29920b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '.', '.'] -> e\n",
      "['.', '.', 'e'] -> m\n",
      "['.', 'e', 'm'] -> m\n",
      "['e', 'm', 'm'] -> a\n",
      "['m', 'm', 'a'] -> .\n",
      "['.', '.', '.'] -> o\n",
      "['.', '.', 'o'] -> l\n",
      "['.', 'o', 'l'] -> i\n",
      "['o', 'l', 'i'] -> v\n",
      "['l', 'i', 'v'] -> i\n",
      "['i', 'v', 'i'] -> a\n",
      "['v', 'i', 'a'] -> .\n",
      "['.', '.', '.'] -> a\n",
      "['.', '.', 'a'] -> v\n",
      "['.', 'a', 'v'] -> a\n",
      "['a', 'v', 'a'] -> .\n",
      "['.', '.', '.'] -> i\n",
      "['.', '.', 'i'] -> s\n",
      "['.', 'i', 's'] -> a\n",
      "['i', 's', 'a'] -> b\n",
      "['s', 'a', 'b'] -> e\n",
      "['a', 'b', 'e'] -> l\n",
      "['b', 'e', 'l'] -> l\n",
      "['e', 'l', 'l'] -> a\n",
      "['l', 'l', 'a'] -> .\n",
      "['.', '.', '.'] -> s\n",
      "['.', '.', 's'] -> o\n",
      "['.', 's', 'o'] -> p\n",
      "['s', 'o', 'p'] -> h\n",
      "['o', 'p', 'h'] -> i\n",
      "['p', 'h', 'i'] -> a\n",
      "['h', 'i', 'a'] -> .\n"
     ]
    }
   ],
   "source": [
    "context_length = 3 \n",
    "X, Y = [], []\n",
    "\n",
    "for word in words[:5]:\n",
    "    # initialize the context for the word: \n",
    "    context  = ['.'] * context_length\n",
    "    for ch in word + '.':\n",
    "        X.append([stoi[s] for s in context])\n",
    "        Y.append(stoi[ch])\n",
    "        print(context, '->', ch)\n",
    "        context =  context[1:] + [ch]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "155af736-75d3-4216-857a-95745fe1031f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0],\n",
      "        [ 0,  0,  5],\n",
      "        [ 0,  5, 13],\n",
      "        [ 5, 13, 13],\n",
      "        [13, 13,  1],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0, 15],\n",
      "        [ 0, 15, 12],\n",
      "        [15, 12,  9],\n",
      "        [12,  9, 22],\n",
      "        [ 9, 22,  9],\n",
      "        [22,  9,  1],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  1],\n",
      "        [ 0,  1, 22],\n",
      "        [ 1, 22,  1],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0,  9],\n",
      "        [ 0,  9, 19],\n",
      "        [ 9, 19,  1],\n",
      "        [19,  1,  2],\n",
      "        [ 1,  2,  5],\n",
      "        [ 2,  5, 12],\n",
      "        [ 5, 12, 12],\n",
      "        [12, 12,  1],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0, 19],\n",
      "        [ 0, 19, 15],\n",
      "        [19, 15, 16],\n",
      "        [15, 16,  8],\n",
      "        [16,  8,  9],\n",
      "        [ 8,  9,  1]])\n",
      "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
      "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/1ct1kks556s6ffsdyjq1grdh0000gn/T/ipykernel_9588/4019588797.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X)\n",
      "/var/folders/ry/1ct1kks556s6ffsdyjq1grdh0000gn/T/ipykernel_9588/4019588797.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y = torch.tensor(Y)\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "403d9ed0-a3ca-41d3-84a1-93192b33e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding matrix: C: 27,2 \n",
    "C = torch.rand(27,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5904672b-56f1-4d44-b74a-060680ff982d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8008e713-a700-438c-b631-9f532596d61d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1681, 0.9909],\n",
       "        [0.1681, 0.9909],\n",
       "        [0.1681, 0.9909]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea6f96c-d546-449a-9308-24aa1dae7e07",
   "metadata": {},
   "source": [
    "Embedding Structure:\n",
    "\n",
    "- `C` is an embedding matrix of `(27, 2)`\n",
    "- `X` is list  of context window, where each context is a list of indicies\n",
    "\n",
    "When we do `C[X]` we get tensor shaped `(32, 3, 2)`, where:\n",
    "- the first dimension (32) is the number of context windows\n",
    "- the second dimension (3) is the number of characters in each context window\n",
    "- the third dimension (2) is the dimensionality of the context window\n",
    "\n",
    "We should view `emb` as 3-dimensional tensor where each \"element\" is a `3,2` of matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94626a67-4954-4fda-9e89-a5843f815e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8dd64667-770a-47ef-a0f4-0c0b1dafe1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1681, 0.9909],\n",
       "        [0.1681, 0.9909],\n",
       "        [0.1681, 0.9909]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d8dd39-9cc0-463e-951e-00b88ce34e03",
   "metadata": {},
   "source": [
    "Neural Networks, particularly MLPs, expect input to be flat vectors. Our new vector represents the entire context window as a single unit of information, which the neural network can process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f7da3da-bcb0-44ba-a5f2-627df699826a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1681, 0.9909, 0.1681, 0.9909, 0.1681, 0.9909])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(-1,6)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be5bc24-23e0-4abd-b2a9-ecd76d33bf76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
