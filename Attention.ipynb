{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnKaGBGgfg8jIPuaYsL0f5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hananather/AutoText/blob/main/Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SelfAttention_v2(nn.Module):\n",
        "  def __init__(self, d_in, d_out, qkv_bias = False):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.W_key= nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "    attn_scores = queries @ keys.T\n",
        "    attn_weights = torch.softmax(\n",
        "        attn_scores / keys.shape[-1]**0.5, dim = -1\n",
        "    )\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "8cjxw7RYnulW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ],
      "metadata": {
        "id": "wtFPgRJCQN_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(789)\n",
        "sa_v2 = SelfAttention_v2(3,2) #n"
      ],
      "metadata": {
        "id": "UjqpAQPqPmZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sa_v2(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlJoHy6ERjA2",
        "outputId": "78e2ed00-fc1c-42bd-dac8-fdc634b25f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0739,  0.0713],\n",
              "        [-0.0748,  0.0703],\n",
              "        [-0.0749,  0.0702],\n",
              "        [-0.0760,  0.0685],\n",
              "        [-0.0763,  0.0679],\n",
              "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Casual Attention"
      ],
      "metadata": {
        "id": "xdGH_k_RUFZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# intutions\n",
        "\n",
        "queries = sa_v2.W_query(inputs)\n",
        "keys = sa_v2.W_key(inputs)\n",
        "attn_scores = queries @ keys.T\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim = -1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21pI2lQtRu4B",
        "outputId": "ace8d867-b6ac-4d36-bd0d-75eb06c1eb6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
            "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
            "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = attn_scores.shape[0]"
      ],
      "metadata": {
        "id": "nGKvkUtkXm4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use tril function to create a mask where the values above diagonal are zero\n",
        "context_length = attn_scores.shape[0]\n",
        "mask_simple = torch.tril(torch.ones(context_length,context_length))\n",
        "mask_simple"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ytttOXbVbNt",
        "outputId": "6fb37b09-138c-433e-a092-770efd4577dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_simple = attn_weights*mask_simple\n",
        "masked_simple"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdXtQBiAYIJL",
        "outputId": "131f8356-2b7a-43ad-80bc-532b0ed2bec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
              "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
              "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we re-normalize\n",
        "row_sums = masked_simple.sum(dim=-1, keepdim = True)\n",
        "print(row_sums)\n",
        "masked_simple_norm = masked_simple / row_sums\n",
        "print(masked_simple_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0VFDIr7Yk3o",
        "outputId": "d00ab2bb-91e0-48b1-b89d-4636dd02d9cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921],\n",
            "        [0.3700],\n",
            "        [0.5357],\n",
            "        [0.6775],\n",
            "        [0.8415],\n",
            "        [1.0000]], grad_fn=<SumBackward1>)\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masking Trick"
      ],
      "metadata": {
        "id": "nsuKipxrbGkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal = 1)\n",
        "print(mask)\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "print(masked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfg2qDEtbJYy",
        "outputId": "3a76662b-b4c1-4aa2-b1d0-8f6f5f2632e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 1., 1., 1., 1., 1.],\n",
            "        [0., 0., 1., 1., 1., 1.],\n",
            "        [0., 0., 0., 1., 1., 1.],\n",
            "        [0., 0., 0., 0., 1., 1.],\n",
            "        [0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0.]])\n",
            "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
            "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
            "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
            "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(masked/ keys.shape[-1]**0.5, dim = 1)\n",
        "attn_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tE2qLtRnbeY3",
        "outputId": "91ee1a87-03b4-4455-91e6-e322ae48b047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
              "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
              "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Causal Attention with Dropout"
      ],
      "metadata": {
        "id": "Iq0b7Bypq_Il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# batch data\n",
        "batch = torch.stack((inputs, inputs), dim = 0)\n",
        "print(batch) # simulating two inputs with six tokens each and embedding dim 3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNFex48MpvSv",
        "outputId": "5af6ce7a-90a6-4e67-b6d0-a1e753d3a6d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.4300, 0.1500, 0.8900],\n",
              "         [0.5500, 0.8700, 0.6600],\n",
              "         [0.5700, 0.8500, 0.6400],\n",
              "         [0.2200, 0.5800, 0.3300],\n",
              "         [0.7700, 0.2500, 0.1000],\n",
              "         [0.0500, 0.8000, 0.5500]],\n",
              "\n",
              "        [[0.4300, 0.1500, 0.8900],\n",
              "         [0.5500, 0.8700, 0.6600],\n",
              "         [0.5700, 0.8500, 0.6400],\n",
              "         [0.2200, 0.5800, 0.3300],\n",
              "         [0.7700, 0.2500, 0.1000],\n",
              "         [0.0500, 0.8000, 0.5500]]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This results in a three-dimensional tensor consisting of two input texts and six tokens each, where each token is  a three-dimensonal embedding vector."
      ],
      "metadata": {
        "id": "dJYWcr8qqkE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dropout = nn.Dropout(0.5) # first we create a dropout obj\n",
        "example = torch.ones(6,6)\n",
        "dropout(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ecn9T3xpsdNb",
        "outputId": "f291f4f4-c789-493a-9e11-669664704696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 2., 2., 0., 2.],\n",
              "        [0., 0., 0., 0., 2., 0.],\n",
              "        [0., 2., 0., 0., 2., 2.],\n",
              "        [2., 0., 2., 2., 2., 0.],\n",
              "        [0., 2., 2., 2., 2., 2.],\n",
              "        [2., 0., 0., 0., 2., 2.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A buffer is a tensor that:\n",
        "1. We want to save with out model. When we save our model to a file, the buffer tensors get saved too.\n",
        "2. We want to automatically move to GPU if we move out model from CPU to GPU\n",
        "3. But we DON'T want to update through trainig (unlike model weights)\n",
        "\n"
      ],
      "metadata": {
        "id": "U2QecEgsGUtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CausalAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    b, num_tokens, d_in = x.shape\n",
        "    keys = self.W_key(x)\n",
        "    values = self.W_value(x)\n",
        "    queries = self.W_queries(x)\n",
        "\n",
        "    attn_scores = queries @ keys.transpose(1,2)\n",
        "    attn_scores.masked_fill_(\n",
        "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
        "    )\n",
        "    attn_weights = torch.softmax(attn_scores/ keys.shape[-1]**0.5, dim = -1)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0AM7xKHPqBW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Efficient Multi-head attention"
      ],
      "metadata": {
        "id": "7FLM8ffZj4RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads,\n",
        "               qkv_bias = False):\n",
        "    self.super().__init_()\n",
        "    assert(d_out  % num_heads == 0), \"d_out  must be divisible by num_heads\"\n",
        "\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads  #1 reduces projection dim\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "\n",
        "    # use Linear layer combine output heads\n",
        "    self.out_proj = nn.Linear(d_in, d_out)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\n",
        "        \"mask\", torch.triu(torch.ones(contex_length, context_length, diagonal =1))\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape\n",
        "\n",
        "    # Tensor shape: (b, num_tokens, d_out)\n",
        "    key = self.W_key(x)\n",
        "    keys = self.W_key(x)\n",
        "    values = self.W_value(x)\n",
        "    queries = self.W_queries(x)\n",
        "\n",
        "    # The view operation is reshaping the data to create multiple 'attention heads,'\n",
        "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "    # transposes from shape (b, num_tokens, num_heads, head_dim)\n",
        "    #                       -> (b, num_heads, num_tokens, head_dim)\n",
        "    keys = keys.transpose(1,2)\n",
        "    queries = queries.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "\n",
        "    # compute the dot product of each head\n",
        "    attn_scores = queries @ keys.transpose(2,3)\n",
        "\n",
        "\n",
        "    # context_length of tokens the model process\n",
        "    # num_tokens is the actual number of tokens in the input sequence\n",
        "    # masks truncated to the number of tokens\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "    attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "    # (b, num_heads, num_tokens, num_tokens)\n",
        "    attn_weights = torch.softmax(\n",
        "        attn_scores/ key.shape[-1]**0.5\n",
        "    )\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    # (b, num_heads, num_tokens, head_dim) ->\n",
        "    # (b, num_heads, head_dim, num_tokens)\n",
        "    context_vec = (attn_weights @ values).transpose(1,2)\n",
        "    context_vec = context_vec.contigouous().view(\n",
        "        b, num_tokens, self.out\n",
        "    )\n",
        "    # not necessary but used in many architectures\n",
        "    context_vec = self.proj(context_vec)\n",
        "    return context_vec\n"
      ],
      "metadata": {
        "id": "Lv9QmjWPj_Pd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shape is (b, num_heads, num_tokens, head_dim ) = (1,2,3,4)\n",
        "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],    #1\n",
        "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
        "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
        "\n",
        "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",
        "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
        "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
        "\n",
        "# a.transpose(2,3): (b, num_heads, head_dim, num_token )\n",
        "# matrix multiplicatin\n",
        "print(a @ a.transpose(2,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMmcYogQe4iq",
        "outputId": "80f7848b-1660-4be3-8b3c-09839d1fd48c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[1.3208, 1.1631, 1.2879],\n",
            "          [1.1631, 2.2150, 1.8424],\n",
            "          [1.2879, 1.8424, 2.0402]],\n",
            "\n",
            "         [[0.4391, 0.7003, 0.5903],\n",
            "          [0.7003, 1.3737, 1.0620],\n",
            "          [0.5903, 1.0620, 0.9912]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_head = a[0,0, :, :]\n",
        "print(first_head)\n",
        "first_res = first_head @ first_head.T\n",
        "print(first_res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNlc5EB7fEIX",
        "outputId": "5eded2be-87e2-4141-8f35-6d5e8740cde3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2745, 0.6584, 0.2775, 0.8573],\n",
            "        [0.8993, 0.0390, 0.9268, 0.7388],\n",
            "        [0.7179, 0.7058, 0.9156, 0.4340]])\n",
            "tensor([[1.3208, 1.1631, 1.2879],\n",
            "        [1.1631, 2.2150, 1.8424],\n",
            "        [1.2879, 1.8424, 2.0402]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rIbevGWngqDW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}